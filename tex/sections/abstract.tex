The differentiable programming paradigm has become a central component of modern machine learning techniques. 
A long tradition of this paradigm exists in the context of scientific computing, in particular in partial differential equation-constrained, gradient-based optimization.
The recognition of the strong conceptual synergies between inverse methods and machine learning offers the opportunity to lay out a coherent framework applicable to both fields.
For models described by differential equations, the calculation of sensitivities and gradients requires careful algebraic and numeric manipulations of the underlying dynamical system.
We aim to summarize some of the most used techniques that exist to compute gradients for numerical solutions of differential equations. 
We cover this problem by first introducing motivations in current areas of research, such as geophysics; the mathematical foundations of the different approaches; and finally the computational consideration and solutions that exist in modern scientific software. 
