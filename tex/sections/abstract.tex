
The differentiable programming paradigm has become a central component of modern machine learning and scientific computing techniques. 
A long tradition of this paradigm exists in the context of inverse methods, in particular in differential equation-constrained, gradient-based optimization.
The recognition of strong synergies between inverse methods and machine learning offers the opportunity to lay out a coherent framework applicable to both fields.
For models described by differential equations, the calculation of the associated gradient loss requires to differentiate the numerical solutions to the differential equation. 
This task is non-trivial and requires careful algebraic and numeric manipulations of the underlying dynamical system.
Here, we provide a comprehensive review of existing techniques to compute derivatives of numerical solutions of differential equation systems.
We first discuss the importance of gradients of solutions of ordinary differential equations in a variety of scientific domains.
% , covering computational fluid dynamics, electromagnetism, geosciences, meteorology, oceanograpgy, climate science, flux inversion, glaciology, solid earth geophysics, biology and ecology, and quantum physics.
Second, we lay out the mathematical foundations of the various approaches and compare them with each other. 
Finally, we cover the computational considerations and explore the solutions available in modern scientific software. 
By delivering an exhaustive review of sensitivity methods, we hope that this work accelerates the fusion of scientific models and data, and fosters a modern approach to scientific modelling.

\vspace{30px}
\noindent \textbf{Key words.} differentiable programming, sensitivity methods, differential equations, inverse modelling, scientific machine learning