The differentiable programming paradigm has become a central component of modern machine learning techniques. 
A long tradition of this paradigm exists in the context of scientific computing, in particular in partial differential equation-constrained, gradient-based optimization.
The recognition of the strong conceptual synergies between inverse methods and machine learning offers the opportunity to lay out a coherent framework applicable to both fields.
For models described by differential equations, the calculation of sensitivities and gradients requires careful algebraic and numeric manipulations of the underlying dynamical system. \todo{Long context for an abstract. We could synthesize this, going more straight to the point of the paper. I would also emphasize on the importance of solution sensitivity in science.}
Here, we provide a comprehensive review of existing techniques to compute gradients of numerical solutions of differential equation systems. \todo{I feel like the motivations are as important as the further derivations of the sensitivity methods. We could consider both as "results".}
We first discuss the importance of gradients of solutions of ODEs in a variety of scientific domains, covering computational fluid dynamics, electromagnetism, geosciences, meteorology, oceanograpgy, climate science, flux inversion, glaciology, solid earth geophysics, biology and ecology, and quantum physics. Second, we lay out the mathematical foundations of the different approaches. Finally, we discuss the computational consideration and solutions that exist in modern scientific software. 
