The continuous adjoint method, also known as continuous adjoint sensitivity analysis (CASA), operates by defining a convenient set of new differential equations for the adjoint variable and using this to compute the gradient in a more efficient manner. 
The continuous adjoint method follows the same logic as the discrete adjoint method, but where the discretization of the differential equation does not happen until the very last step, when the solutions of the differential equations need to be solve numerically. 
The Lagrangian derivation of the continuous adjoint method can also be found in Appendix \ref{appendix:lagrangian}.

Consider an integrated loss function defined in Equation \eqref{eq:integrated-loss-function} of the form 
\begin{equation}
    L(u; \theta) = \inttime h(u(t;\theta), \theta) dt
\end{equation}
and its derivative with respect to the parameter $\theta$ given by the following integral involving the sensitivity matrix $s(t)$:
\begin{equation}
    \frac{dL}{d\theta}
    = 
    \inttime \left( \frac{\partial h}{\partial \theta} + \frac{\partial h}{\partial u} s(t) \right) dt.
    \label{eq:casa-loss}
\end{equation}
Just as in the case of the discrete adjoint method, the complicated term to evaluate in the last expression is the sensitivity (Equation \eqref{eq:sensitivity-definition}).
Again, the trick is to evaluate the VJP $\frac{\partial h}{\partial u} \frac{\partial u}{\partial \theta}$ by first defining an intermediate adjoint variable. 
The continuous adjoint equation now is obtained by finding the dual/adjoint equation associated to the forward sensitivity equation using the weak formulation of Equation \eqref{eq:sensitivity_equations}. 
The adjoint equation is obtained by writing the forward sensitivity equation in the form 
\begin{equation}
    \inttime \lambda(t)^T \left( \frac{ds}{dt} - \frac{\partial f}{\partial u} \, s - \frac{\partial f}{\partial \theta}  \right) dt 
    = 
    0,
    \label{eq:integrated-sensitivity-equation}
\end{equation}
where this equation must be satisfied for every function $\lambda(t)$ in order for Equation \eqref{eq:sensitivity_equations} to be true. 
The next step is to get rid of the time derivative applied to the sensitivity $s(t)$ using integration by parts: 
\begin{equation}
    \inttime \lambda(t)^T \frac{ds}{dt} dt
    = 
    \lambda(t_1)^T s(t_1) - \lambda(t_0)^T s(t_0)
    -
    \inttime \frac{d\lambda^T}{dt} s(t)\, dt.
\end{equation}
Replacing this last expression into Equation \eqref{eq:integrated-sensitivity-equation} we obtain 
\begin{equation}
    \inttime \left( - \frac{d\lambda^T}{dt} -  \lambda(t)^T \frac{\partial f}{\partial u} \right) s(t) dt
    =
    \inttime \lambda(t)^T \frac{\partial f}{\partial \theta} dt 
    - 
    \lambda(t_1)^T s(t_1)
    + 
    \lambda(t_0)^T s(t_0).
    \label{eq:casa-semiadjoint}
\end{equation}
At first glance, there is nothing particularly interesting about this last equation. 
However, both Equations \eqref{eq:casa-loss} and \eqref{eq:casa-semiadjoint} involve a VJP with $s(t)$. 
Since Equation \eqref{eq:casa-semiadjoint} must hold for every function $\lambda(t)$, we can pick $\lambda(t)$ to make the terms involving $s(t)$ in Equations \eqref{eq:casa-loss} and \eqref{eq:casa-semiadjoint} to perfectly coincide. 
This is done by defining the adjoint $\lambda(t)$ to be the solution of the new system of differential equations
\begin{equation}
    \frac{d\lambda}{dt} 
    = 
    - 
    \frac{\partial f}{\partial u}^T \lambda  
    - 
    \frac{\partial h^T}{\partial u} 
    \qquad \quad \lambda(t_1) = 0. 
    \label{eq:casa-adjoint-equation}
\end{equation}
Notice that the adjoint equation is defined with the final condition at $t_1$, meaning that it needs to be solved backwards in time. 
The definition of the adjoint $\lambda(t)$ as the solution of this last ODE simplifies Equation \eqref{eq:casa-semiadjoint} to
\begin{equation}
    \inttime \frac{\partial h}{\partial u} s(t) dt
    = 
    \lambda(t_0)^T s(t_0)
    + 
    \inttime \lambda(t)^T \frac{\partial f}{\partial \theta} dt.
\end{equation}
Finally, replacing this inside the expression for the gradient of the loss function we have 
\begin{equation}
    \frac{dL}{d\theta}
    = 
    \lambda(t_0)^T s(t_0)
    + 
    \inttime
    \left( \frac{\partial h}{\partial \theta} + \lambda^T \frac{\partial f}{\partial \theta} \right) dt
    \label{eq:casa-final-loss-gradient}
\end{equation}
The full algorithm to compute the full gradient $\frac{dL}{d\theta}$ can be described as follows: (i) Solve the original differential equation $\frac{du}{dt} = f(u, t, \theta)$, (ii) Solve the backwards adjoint differential equation given by Equation \eqref{eq:casa-adjoint-equation}, (iii) Compute the gradient using Equation \eqref{eq:casa-final-loss-gradient}.
% \begin{enumerate}[label=(\roman*)]
%     \item Solve the original differential equation $\frac{du}{dt} = f(u, t, \theta)$;
%     \item Solve the backwards adjoint differential equation given by Equation \eqref{eq:casa-adjoint-equation};
%     \item Compute the gradient using Equation \eqref{eq:casa-final-loss-gradient}.
% \end{enumerate}



