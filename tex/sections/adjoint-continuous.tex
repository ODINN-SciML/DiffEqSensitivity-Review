The continuous adjoint method, also known as continuous adjoint sensitivity analysis (CASA), operates by defining a convenient set of new differential equations for the adjoint variable and using this to compute the gradient in a more efficient manner. 
Mathematically speaking, the adjoint equations can be derived from a duality or Lagrangian point of view \cite{Giles_Pierce_2000}.
We prefer to derive the equation using the former methods since we believe it gives better insights to how the method works and also allow to generalize to other user cases. 
The derivation of both the discrete and continuous adjoint methods using Lagrangian multipliers can be found in Appendix \ref{appendix:lagrangian}.
We encourage the interested reader to make the effort of following how the continuous adjoint method follows the same logic than the discrete methods, but where the discretization of the differential equation does not happen until the very last step, when the solutions of the differential equations involved need to be numerically evaluated. 
%\footnote{Based on slides of Chris Rackauckas about "Data Efficient model discovery with Scientific Machine Learning", Neural ODE paper, video "The use and practice of scientific machine learning". Paper comparing CSA and DSA} 

Consider an integrated loss function of the form 
\begin{equation}
    L(u; \theta) = \inttime h(u(t;\theta), \theta) dt
\end{equation}
and its derivative with respect to the parameter $\theta$ given by
\begin{equation}
    \frac{dL}{d\theta}
    = 
    \inttime \left( \frac{\partial h}{\partial \theta} + \frac{\partial h}{\partial u} s(t) \right) dt.
    \label{eq:casa-loss}
\end{equation}
As explained in previous section, the complicated term to evaluate in the last expression is the sensitivity (Equation \eqref{eq:sensitivity-definition}).
Just as in the case of the discrete adjoint method, the trick is to evaluate the VJP $\frac{\partial h}{\partial u} \frac{\partial u}{\partial \theta}$ by defining an intermediate adjoint variable. 
The continuous adjoint equation now is obtained by finding the dual/adjoint equation of the sensitivity equation using the weak formulation of Equation \eqref{eq:sensitivity_equations}. 
The adjoint equation is obtained by writing the sensitivity equation in the form 
\begin{equation}
    \inttime \lambda(t)^T \left( \frac{ds}{dt} - f(u, \theta, t) \, s - \frac{\partial f}{\partial \theta}  \right) dt 
    = 
    0,
    \label{eq:integrated-sensitivity-equation}
\end{equation}
where this equation must be satisfied for every function $\lambda(t)$ in order to Equation \eqref{eq:continuous-adjoint} to be true. 
The next step is to get rid of all time derivative applied to the sensitivity $s(t)$ using integration by parts: 
\begin{equation}
    \inttime \lambda(t)^T \frac{ds}{dt} dt
    = 
    \lambda(t_1)^T s(t_1) - \lambda(t_0)^T s(t_0)
    -
    \inttime \frac{d\lambda^T}{dt} s(t)\, dt.
\end{equation}
Replacing this last expression into Equation \eqref{eq:integrated-sensitivity-equation} we obtain 
\begin{equation}
    \inttime \left( - \frac{d\lambda^T}{dt} -  \lambda(t)^T f(u, \theta, t) \right) s(t) dt
    =
    \inttime \lambda(t)^T \frac{\partial f}{\partial \theta} dt 
    - 
    \lambda(t_1)^T s(t_1)
    + 
    \lambda(t_0)^T s(t_0).
    \label{eq:casa-semiadjoint}
\end{equation}
At first glance, there is nothing particularly interesting about this last equation. 
However, both Equations \eqref{eq:casa-loss} and \eqref{eq:casa-semiadjoint} involved a VJP with $s(t)$. 
Since Equation \eqref{eq:casa-semiadjoint} must hold for every function $\lambda(t)$, we can pick $\lambda(t)$ to make the terms involving $s(t)$ in Equations \eqref{eq:casa-loss} and \eqref{eq:casa-semiadjoint} to perfectly coincide. 
This is done by defining the adjoint $\lambda(t)$ to be the solution of the new system of differential equations
\begin{equation}
    \frac{d\lambda}{dt} 
    = 
    - 
    f(u, \theta, t)^T \lambda  
    - 
    \frac{\partial h^T}{\partial u} 
    \qquad \quad \lambda(t_1) = 0. 
    \label{eq:casa-adjoint-equation}
\end{equation}
Notice that the adjoint equation is define with final condition at $t_1$, meaning that it needs to be solved backwards in time. 
The definition of the adjoint $\lambda(t)$ as the solution of this last ODE simplifies Equation \eqref{eq:casa-semiadjoint} to
\begin{equation}
    \inttime \frac{\partial h}{\partial u} s(t) dt
    = 
    \lambda(t_0)^T s(t_0)
    + 
    \inttime \lambda^T (t) \frac{\partial f}{\partial \theta} dt.
\end{equation}
Finally, replacing this inside the expression for the gradient of the loss function we have 
\begin{equation}
    \frac{dL}{d\theta}
    = 
    \lambda^T(t_0) s(t_0)
    + 
    \inttime
    \left( \frac{\partial h}{\partial \theta} + \lambda^T \frac{\partial f}{\partial \theta} \right) dt
    \label{eq:casa-final-loss-gradient}
\end{equation}
The full algorithm to compute the full gradient $\frac{dL}{d\theta}$ can be described as follows:
\begin{enumerate}
    \item Solve the original differential equation $\frac{du}{dt} = f(u, t, \theta)$;
    \item Solve the backwards adjoint differential equation \eqref{eq:casa-adjoint-equation};
    \item Compute the gradient using Equation \eqref{eq:casa-final-loss-gradient}.
\end{enumerate}
The bottleneck on this method is the calculation of the adjoint, since in order to solve the adjoint equation we need to know $u(t)$ at any given time. 
Effectively, notice that the adjoint equation involves the term $f(u, \theta, t)$ and $\frac{\partial h}{\partial u}$ which are both functions of $u(t)$. 
There are different ways of addressing the evaluation of $u(t)$ during the backward step:
\begin{enumerate}[label=(\roman*)]
    \item Solve for $u(t)$ again backwards.
    \item Store $u(t)$ in memory during the forward step.
    \item Store reference values in memory and interpolate in between. 
    This technique is known as \textit{checkpoiting} or windowing that tradeoffs computational running time and storage.
    This is implemented in \texttt{Checkpoiting.jl} \cite{Checkpoiting_2023}.
\end{enumerate} 
Computing the ODE backwards can be unstable and lead to exponential errors \cite{kim_stiff_2021}. 
In \cite{chen_neural_2019}, the solution is recalculated backwards together with the adjoint simulating an augmented dynamics: 
\begin{equation}
    \frac{d}{dt}
    \begin{bmatrix}
       u \\
       \lambda \\
       \frac{dL}{d\theta}
    \end{bmatrix}
    = 
    \begin{bmatrix}
       -f \\
       - \lambda^T \frac{\partial f}{\partial u} \\
       - \lambda^T \frac{\partial f}{\partial \theta}
    \end{bmatrix}
    = 
    - [ 1, \lambda^T, \lambda^T ]
    \begin{bmatrix}
       f & \frac{\partial f}{\partial u} & \frac{\partial f}{\partial \theta} \\
       0 & 0 & 0 \\
       0 & 0 & 0
    \end{bmatrix},
\end{equation}
with initial condition $[u(t_1), \frac{\partial L}{\partial u(t_1)}, 0]$. One way of solving this system of equations that ensures stability is by using implicit methods. However, this requires cubic time in the total number of ordinary differential equations, leading to a total complexity of $\mathcal O((n+p)^3)$ for the adjoint method. Two alternatives are proposed in \cite{kim_stiff_2021}, the first one called \textit{Quadrature Adjoint} produces a high order interpolation of the solution $u(t)$ as we move forward, then solve for $\lambda$ backwards using an implicit solver and finally integrating $\frac{dL}{d\theta}$ in a forward step. This reduces the complexity to $\mathcal O (n^3 + p)$, where the cubic cost in the number of ODEs comes from the fact that we still need to solve the original stiff differential equation in the forward step. A second but similar approach is to use a implicit-explicit (IMEX) solver, where we use the implicit part for the original equation and the explicit for the adjoint. This method also will have complexity $\mathcal O (n^3 + p)$.