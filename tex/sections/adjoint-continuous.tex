The continuous adjoint method, also known as continuous adjoint sensitivity analysis (CASA), operates by defining a convenient set of new differential equations for the adjoint variable and using this to compute the gradient in a more efficient manner. 
We encourage the interested reader to make the effort of following how the continuous adjoint method follows the same logic than the discrete methods, but where the discretization of the differential equation does not happen until the very last step, when the solutions of the differential equations involved need to be numerically evaluated. 
%\footnote{Based on slides of Chris Rackauckas about "Data Efficient model discovery with Scientific Machine Learning", Neural ODE paper, video "The use and practice of scientific machine learning". Paper comparing CSA and DSA} 

Consider an integrated loss function of the form 
\begin{equation}
    L(u; \theta) = \inttime h(u(t;\theta), \theta) dt
\end{equation}
and its derivative with respect to the parameter $\theta$ given by the following integral involving the sensitivity matrix $s(t)$:
\begin{equation}
    \frac{dL}{d\theta}
    = 
    \inttime \left( \frac{\partial h}{\partial \theta} + \frac{\partial h}{\partial u} s(t) \right) dt.
    \label{eq:casa-loss}
\end{equation}
Just as in the case of the discrete adjoint method, the complicated term to evaluate in the last expression is the sensitivity (Equation \eqref{eq:sensitivity-definition}).
Just as in the case of the discrete adjoint method, the trick is to evaluate the VJP $\frac{\partial h}{\partial u} \frac{\partial u}{\partial \theta}$ by defining an intermediate adjoint variable. 
The continuous adjoint equation now is obtained by finding the dual/adjoint equation of the sensitivity equation using the weak formulation of Equation \eqref{eq:sensitivity_equations}. 
The adjoint equation is obtained by writing the sensitivity equation in the form 
\begin{equation}
    \inttime \lambda(t)^T \left( \frac{ds}{dt} - f(u, \theta, t) \, s - \frac{\partial f}{\partial \theta}  \right) dt 
    = 
    0,
    \label{eq:integrated-sensitivity-equation}
\end{equation}
where this equation must be satisfied for every function $\lambda(t)$ in order for Equation \eqref{eq:continuous-adjoint} to be true. 
The next step is to get rid of all time derivative applied to the sensitivity $s(t)$ using integration by parts: 
\begin{equation}
    \inttime \lambda(t)^T \frac{ds}{dt} dt
    = 
    \lambda(t_1)^T s(t_1) - \lambda(t_0)^T s(t_0)
    -
    \inttime \frac{d\lambda^T}{dt} s(t)\, dt.
\end{equation}
Replacing this last expression into Equation \eqref{eq:integrated-sensitivity-equation} we obtain 
\begin{equation}
    \inttime \left( - \frac{d\lambda^T}{dt} -  \lambda(t)^T f(u, \theta, t) \right) s(t) dt
    =
    \inttime \lambda(t)^T \frac{\partial f}{\partial \theta} dt 
    - 
    \lambda(t_1)^T s(t_1)
    + 
    \lambda(t_0)^T s(t_0).
    \label{eq:casa-semiadjoint}
\end{equation}
At first glance, there is nothing particularly interesting about this last equation. 
However, both Equations \eqref{eq:casa-loss} and \eqref{eq:casa-semiadjoint} involved a VJP with $s(t)$. 
Since Equation \eqref{eq:casa-semiadjoint} must hold for every function $\lambda(t)$, we can pick $\lambda(t)$ to make the terms involving $s(t)$ in Equations \eqref{eq:casa-loss} and \eqref{eq:casa-semiadjoint} to perfectly coincide. 
This is done by defining the adjoint $\lambda(t)$ to be the solution of the new system of differential equations
\begin{equation}
    \frac{d\lambda}{dt} 
    = 
    - 
    f(u, \theta, t)^T \lambda  
    - 
    \frac{\partial h^T}{\partial u} 
    \qquad \quad \lambda(t_1) = 0. 
    \label{eq:casa-adjoint-equation}
\end{equation}
Notice that the adjoint equation is defined with the final condition at $t_1$, meaning that it needs to be solved backwards in time. 
The definition of the adjoint $\lambda(t)$ as the solution of this last ODE simplifies Equation \eqref{eq:casa-semiadjoint} to
\begin{equation}
    \inttime \frac{\partial h}{\partial u} s(t) dt
    = 
    \lambda(t_0)^T s(t_0)
    + 
    \inttime \lambda(t)^T \frac{\partial f}{\partial \theta} dt.
\end{equation}
Finally, replacing this inside the expression for the gradient of the loss function we have 
\begin{equation}
    \frac{dL}{d\theta}
    = 
    \lambda(t_0)^T s(t_0)
    + 
    \inttime
    \left( \frac{\partial h}{\partial \theta} + \lambda^T \frac{\partial f}{\partial \theta} \right) dt
    \label{eq:casa-final-loss-gradient}
\end{equation}
The full algorithm to compute the full gradient $\frac{dL}{d\theta}$ can be described as follows:
\begin{enumerate}
    \item Solve the original differential equation $\frac{du}{dt} = f(u, t, \theta)$;
    \item Solve the backwards adjoint differential equation \eqref{eq:casa-adjoint-equation};
    \item Compute the gradient using Equation \eqref{eq:casa-final-loss-gradient}.
\end{enumerate}


\subsubsection{Are discrete and continuous adjoint the same?}

As we explored in the previous sections, discrete adjoints can be derived by discrediting the continuous adjoint. 
The difference between the two is which one between discretization and differentiation happens first. 
A natural question then is if these two methods effectively compute the same sensitivity, meaning that the discrete adjoint consistently approximate its continuous counterpart. 
Different works exist regarding the consistency or inconsistency between the two approaches, and this usually depends on the ODE and equation.  
Proofs of the consistency of discrete adjoint methods for Runge-Kutta methods have been provided in \cite{sandu2006properties, sandu2011solution}.
Depending on the choice of the Runge-Kutta coefficients, we can have a numerical scheme that is both consistent for the original equation and consistent/inconsistent for the adjoint \cite{Hager_2000}.
Furthermore, adjoint methods can be completely fail in chaotic systems \cite{Wang2012-chaos-adjoint}.

In the more general case, both methods can give different computational results \cite{Sirkes_Tziperman_1997}.
In \cite{Sirkes_Tziperman_1997} it is shown that discrete adjoints can lead to unstabilites and wrong gradients.
