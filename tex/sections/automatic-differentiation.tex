% I think I need some historical reference here.
Automatic differentiation (AD) is a technology that generates new code representing derivatives of a given parent code. 
Examples are code representing the tangent linear or adjoint operator of the parent code
\cite{Griewank:2008kh}. 
The names \textit{algorithmic} and \textit{computational} differentiation had also been used in the literature, emphasizing the algorithmic rather than automatic nature of AD \cite{Griewank:2008kh}. 
The basis of all AD systems is the notion that complicated functions included in any computer program can be reduced to a sequence of simple algebraic operations that have straightforward derivative expressions, based upon elementary rules of differentiation \cite{juedes1991taxonomy}.
The derivatives of the outputs of the computer program (dependent variables) with respect to their inputs (independent variables) are then combined using the chain rule.
One advantage of AD systems is to automatically differentiate programs that include control flow, such as branching, loops or recursions. 
This is because any program can be reduced to a trace of input, intermediate and output variables \cite{Baydin_Pearlmutter_Radul_Siskind_2015}.

Depending if the concatenation of these gradients is done as we execute the program (from input to output) or in a later instance where we trace-back the calculation from the end (from output to input), we refer to \textit{forward} or \textit{reverse} AD, respectively.
Neither forward nor reverse mode is more efficient in all cases \cite{Griewank_1989}, as we will discuss in Section \ref{sec:vjp-jvp}.

\subsubsection{Forward mode}

Forward mode AD amounts to computing directional derivatives. It can be implemented in different ways depending on the data structures we use at the moment of representing a computer program. Examples of these data structures include dual numbers and Wengert lists \cite{Baydin_Pearlmutter_Radul_Siskind_2015}.

\paragraph{Dual numbers}
\label{section:dual-numbers}
% \vspace*{10px}
% \noindent \textbf{\textit{Dual numbers}}
% \vspace*{5px}

Dual numbers extend the definition of a numerical variable that takes a certain value to also carry information about its derivative with respect to a certain parameter \cite{clifford1871dualnumbers}. 
We can define an abstract type, defined as a dual number, composed of two elements: a \textit{value} coordinate $x_1$ that carries the value of the variable and a \textit{derivative} coordinate $x_2$ with the value of the derivative $\frac{\partial x_1}{\partial \theta}$. 
Just as complex number, we can represent dual numbers as an ordered pair $(x_1, x_2)$, sometimes known as Argand pair, or in the rectangular form 
\begin{equation}
 x_\epsilon = x_1 + \epsilon \, x_2
\end{equation}
where $\epsilon$ is an abstract number called a perturbation or tangent, with the properties $\epsilon^2 = 0$ and $\epsilon \neq 0$.
This last representation is quite convenient since it naturally allow us to extend algebraic operations, like addition and multiplication, to dual numbers \cite{Karczmarczuk2001}. 
For example, given two dual numbers $x_\epsilon = x_1 + \epsilon x_2$ and $y_\epsilon = y_1 + \epsilon y_2$, it is easy to derive using the fact $\epsilon^2=0$ that
\begin{equation}
 x_\epsilon + y_\epsilon = (x_1 + y_1) + \epsilon \, (x_2 + y_2)
 \qquad
 x_\epsilon y_\epsilon = x_1 y_1 + \epsilon \, (x_1 y_2 + x_2 y_1) .
 %\qquad
 %\frac{x_\epsilon}{y_\epsilon} = \frac{x_1}{y_1} + \epsilon \, \frac{x_2 y_1 - x_1 y_2}{y_1^2}.
\end{equation}
From these last examples, we can see that the derivative component of the dual number carries the information of the derivatives when combining operations.
For example, suppose than in the last example the dual variables $x_2$ and $y_2$ carry the value of the derivative of $x_1$ and $x_2$ with respect to a parameter $\theta$, respectively. 

Intuitively, we can think of $\epsilon$ as being a differential in the Taylor series expansion, fact that we can observe in how the output of any scalar functions is extended to a dual number output:
\begin{align}
\begin{split}
    f(x_1 + \epsilon x_2)
    &= 
    f(x_1)
    + 
    \epsilon \, x_2 \,  f'(x_1)
    + 
    \epsilon^2 \cdot ( \ldots )\\
    &= 
    f(x_1)
    + 
    \epsilon \, x_2 \,  f'(x_1).
\end{split}
\label{eq:dual-number-function}
\end{align}
When computing first order derivatives, we can ignore everything of order $\epsilon^2$ or larger, which is represented in the condition $\epsilon^2 = 0$.
This implies that we can use dual numbers to implement forward AD through a numerical algorithm. 
In Section \ref{sec:computational-implementation} we will explore how this is implemented and compare this approach with complex-step differentiation. 

Multidimensional dual number generalize dual number to include a different dual variable $\epsilon_i$ for each variable we want to differentiate with respect to\cite{Neuenhofen_2018, RevelsLubinPapamarkou2016}.
A multidimensional dual number is then defined as $x_\epsilon = x + \sum_{i=1}^p x_i \epsilon_i$ with $\epsilon_i \epsilon_j = 0$ for all pairs $i$ and $j$.
% Notice that a major limitation of the dual number approach is that we need a dual variable for each variable we want to differentiate. 
Incorrect implementations of this aspect can lead to \textit{perturbation confusion} \cite{siskind2005perturbation, manzyuk2019perturbation}, an existing problem in some AD software where dual variables corresponding to different variables result indistinguishable, especially in the case of nested functions \cite{manzyuk2019perturbation}.   
Another extension of dual numbers that should not be confused with multidimensional dual numbers are hyper-dual numbers, which allow to compute higher-order derivatives of a function \cite{fike2013multi}. 
% Here we need to say that for p > 1 the dual component is carrying the value of the sensitivity!!! This will make the connection with sensitivity equations more explicit. 

\paragraph{Computational graph}
% \vspace*{10px}
% \noindent \textbf{\textit{Computational graph}}
% \vspace*{5px}

A useful way of representing a computer program is via a computational graph with intermediate variables that relate the input and output variables. 
Most scalar functions of interest can be represented in this factorial form as a acyclic directed graph with nodes associated to variables and edges to atomic operations \cite{Griewank:2008kh, Griewank_1989}, known as Kantorovich graph \cite{kantorovich1957mathematical} or Wengert trace/tape\cite{Wengert_1964, Bauer_1974}. 
% Although notation can be a little bit difficult to digest here, the mathematics behind is rather simple. 
We can define $v_1, v_2, \ldots, v_p = \theta_1, \theta_2, \ldots, \theta_p$ the input set of variables; $v_{p+1}, \ldots, v_{m-1}$ the set of all the intermediate variables, and finally $v_m = L(\theta)$ the final output of a computer program. 
This can be done in such a way that the order is strict, meaning that each variable $v_i$ is computed just as a function of the previous variables $v_j$ with $j < i$. 
Once the graph is constructed, we can compute the derivative of every node with respect to other (a quantity known as the tangent) using the Bauer formula \cite{Bauer_1974, Oktay_randomized-AD}
\begin{equation}
    \frac{\partial v_j}{\partial v_i}
    = 
    \sum_{\substack{ \text{paths }w_0 \rightarrow w_1 \rightarrow \ldots \rightarrow w_K \\
                    \text{with } w_0=v_i, w_K = v_j}}
    \prod_{k=0}^{K-1} \frac{\partial w_{k+1}}{\partial w_{k}},
\end{equation}
where the sum is calculated with respect to all the directed paths in the graph connecting the input and target node.
Instead of evaluating the last expression for all possible path, a simplification is to increasingly evaluate $j=p+1, \ldots, m$ using the recursion 
\begin{equation}
    \frac{\partial v_j}{\partial v_i}
    = 
    \sum_\text{$w$\text{ such that} $w \rightarrow v_j$}
    \frac{\partial v_j}{\partial w}
    \frac{\partial w}{\partial v_i} 
    \label{eq:AD-graph-recursion}
\end{equation}
Since every variable node $w$ such that $w \rightarrow v_j$ is an edge of the computational graph have index less than $j$, we can iterate this procedure as we run the computer program and solve for both the function and its gradient.
This is possible because in forward mode the term $\frac{\partial w}{\partial v_i}$ has been computed in a previous iteration, while $\frac{\partial v_j}{\partial w}$ can be evaluated at the same time the node $v_j$ is computed based on only the value of the parent variable nodes. 
The only requirement for differentiation is being able to compute the derivative/tangent of each edge/primitive and combine these using the recursion \eqref{eq:AD-graph-recursion}.

\subsubsection{Reverse mode}

Reverse mode AD is also known as the adjoint of cotangent linear mode, or backpropagation in the field of machine learning. 
The reverse mode of automatic differentiation has been introduced in different contexts \cite{griewank2012invented} and materializes the observation made by Phil Wolfe that if the chain rule is implemented in reverse mode, then the ratio between the computation of the gradient of a function and the function itself can be bounded by a constant that does not depend of the number of parameters to differentiate \cite{Griewank_1989, Wolfe_1982}, a point known as the \textit{cheap gradient principle} \cite{griewank2012invented}.  
Given a directional graph of operations defined by a Wengert list \cite{Wengert_1964}, we can compute gradients of any given function in the same fashion as Equation \eqref{eq:AD-graph-recursion} but in reverse mode as
\begin{equation}
 \bar v_i = \frac{\partial \ell}{\partial v_i}= \sum_{w : v \rightarrow w \in G} \frac{\partial w}{\partial v} \bar{w}.
\end{equation}
In this context, the notation $\bar{\omega} = \frac{\partial \ell}{\partial \omega}$ is introduced to signify the partial derivative of the final loss function with respect to various program variables. 
This derivative is often referred to as the adjoint, dual, or cotangent, and its connection with the discrete adjoint method will be made more explicitly in Section \ref{section:comparison-discrete-adjoint-AD}. 

Since in reverse-mode AD the values of $\bar \omega$ are being updated in reverse order, in general
% , i.e., for nonlinear function evaluations or in the presence of complex flow graphs, 
we need to know the state value of all the argument variables $v$ of $\omega$ in order to evaluate the terms $\frac{\partial \omega}{\partial v}$.
These state values (required variables) need to be either stored in memory during the evaluation of the function or recomputed ``on the fly'' in order to be able to evaluate the derivative. 
Checkpointing schemes exist to limit and balance the amount of soring versus recomputation (see section \ref{section:checkpointing}).


\subsubsection{AD connection with JVPs and VJPs}
\label{sec:vjp-jvp}

When working with unit operations that involve matrix operations dealing with vectors of different dimensions, the order in which we apply the chain rule matters \cite{Giering_Kaminski_1998}.
When computing a gradient using AD, we can encounter vector-Jacobian products (VJPs) or Jacobian-vector products (JVP).
As their name indicates, the difference between them is that the quantity we are interested in is described by the product of a Jacobian times a vector on the left side (VJP) or the right (JVP).

For nested functions, the Jacobian is described as the product of multiple Jacobians using the chain rule.
In this case, the full gradient is computed as the chain product of vectors and Jacobians. 
Let us consider for example the case of a loss function $L : \mathbb R^p \mapsto \mathbb R$ taking a total of $p$ arguments as inputs that can be decomposed as $L(\theta) = \ell \circ g_{k} \circ \ldots \circ g_2 \circ g_1(\theta)$, with $\ell : \mathbb R^{d_k} \mapsto \mathbb R$ the final evaluation of the loss function after we apply in order a sequence of intermediate functions $g_i : \mathbb R^{d_{i-1}} \mapsto \mathbb R^{d_i}$, where we define $d_0 = p$ for simplicity. 
Using the chain rule states that the gradient of the final loss function is
\begin{equation}
 \nabla_\theta L = \nabla \ell \cdot Dg_{k} \cdot Dg_{k-1} \cdot \ldots \cdot Dg_2 \cdot Dg_1, 
\end{equation}
with $Dg_i$ the Jacobians of each nested function evaluated at the intermediate values $g_{i-1} \circ g_{i-2} \circ \ldots \circ g_i (\theta)$.
Notice that in the last equation, $\nabla \ell \in \mathbb R^{d_k}$ is a vector.
In order to compute $\nabla_\theta L$, we can solve the multiplication starting from the right side, which will correspond to multiplying the Jacobians forward from $Dg_1$ to $Dg_k$, or from the left side, moving backwards. 
The important aspect of the backwards case is that we will always be computing VJPs, since $\nabla \ell$ is a vector.
Since VJPs are easier to evaluate than full Jacobians, the reverse mode will in general be faster when $1 \ll p$.
To illustrate this, let us consider the following example displayed in Figure \ref{fig:vjp-jvp}. 
For general rectangular matrices $A\in \mathbb R^{d_1 \times d_2}$ and $B \in \mathbb R^{d_2 \times d_3}$, the cost of the matrix multiplication $AB$ is $\mathcal O (d_1 d_2 d_3)$.
It is worth noticing that if well more efficient methods for matrix-matrix multiplication based on Strassen’s recursive algorithm and its variants exist, these are not extensively used in most scientific applications \cite{Silva_Gustafson_Wong_2018, Huang_Smith_Henry_Geijn_2016}.
This implies that forward AD requires a total of
\begin{equation}
 d_2 d_1 p + d_3 d_2 p + \ldots + d_k d_{k-1} p + d_k p = \mathcal O (kp)
\end{equation}
operations, while backwards mode AD requires
\begin{equation}
 d_k d_{k-1} + d_{k-1} d_{k-2} + \ldots + d_2 d_1 + d_1 p = \mathcal O (k+p)
\end{equation}
operations, where the $\mathcal O$ is with respect to the variable $p$. 

In the general case of a function $L : \R^p \mapsto \R^q$ with multiple outputs and a total of $k$ intermediate functions, the cost of forward AD is $\mathcal O (pk + q)$ and the cost of reverse is $\mathcal O (p + kq)$.
When the function to differentiate has a larger input space than output ($q \ll p$), AD in reverse mode is more efficient as it propagates the chain rule by computing VJPs, the reason why reverse-mode AD is more used in modern machine learning.
% On the other side, when the output dimension is larger than the input space dimension, forwards AD is more efficient.
% This is the reason why in most machine learning application people use backwards AD. 
However, notice that backwards mode AD requires us to save intermediate variables through the forward run in order to run backwards afterwards \cite{Bennett_1973}, leading to performance overhead that makes forward AD more efficient when $p \lesssim q$ \cite{Griewank_1989, Margossian_2018, Baydin_Pearlmutter_Radul_Siskind_2015}.  
In other words, backwards AD is really more efficient when $q \ll p$. 
% while in forward mode we can just evaluate the gradient as we iterate our sequence of functions. 
We discuss in section XXX how this problem can be overcome with a good checkpointing scheme. 
% On the other hand, when the goal is to compute the gradient of many function outputs with respect to a few parameters, forward mode AD is more efficient \cite{Griewank_1989}.
% This means that for problems with a small number of parameters, forward mode can be faster and more memory-efficient that backwards AD.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{tex/figures/VJP-AD.pdf}
    \caption{Comparison between forward and backward AD. Changing the order of Jacobian multiplications changes the total number of floating-point operations, which leads to different computational complexities between forward and reverse mode. When the multiplication is carried from the right side of the mathematical expression for $\nabla_\theta L$, each matrix simplification involves a matrix with size $p$, giving a total complexity of $\mathcal O (kp)$. This is the opposite of what happens when we carried the VJP from the left side of the expression, where the matrix of size $d_1 \times p$ has no effect in the intermediate calculations, making all the intermediate calculations $\mathcal O (1)$ with respect to $p$ and a total complexity of $\mathcal O (k + p)$. }
    % However, backwards mode requires storing in memory information about the forward execution of the program, while forward mode can update the gradient on running time.}
    \label{fig:vjp-jvp}
\end{figure}
