% Introduction paragram with model based 
Models based on ordinary differential equations (ODEs) and more generally differential equations (DE) play a central role in describing the behaviour of mechanistic systems in natural and social sciences \cite{Ghattas.2021}.
They consist of precise mathematical descriptions of physical mechanisms, including the modelling of causal interactions, feedback loops, and dependencies between components of the system under consideration \cite{rackauckas2020universal}.
% Differential equations are the cornerstone of the physical sciences.
Examples of this include the modelling of the evolution of atmospheric and ocean currents and ice flow in geophysics, evolution of species in ecology, etc.  
For centuries, the scientific community relied only on theoretical and analytical approaches to model and solve systems governed by differential equations. 
The introduction of numerical methods and, most importantly, the development of the computer, allowed scientists to enter the realm of computational modelling \cite{hey2009}. 
With such numerical methods to approximate their solutions, ODE-based models led to fundamental advances in the understanding and prediction of physical, biological, and social systems, among others.

% In a more abstract sense, the solution of differential equations can be seen as functions that map model parameters parameter and initial conditions to state variables and observations, similar to forward models, statistics, and even machine learning. 

% General statement: why gradients are important?
Quantifying how much the output of a ODE-based model changes with respect to its arguments and parameters, also known as model sensitivity, is the cornerstone of many scientific computing and machine learning methods. 
This includes applications to optimization, sensitivity analysis, Bayesian inference, inverse methods, and uncertainty quantification, among many \cite{Razavi.2021}. 
In most cases, the model sensitivity can be computed by the evaluation of one or multiple gradients of the model output. 
In sensitivity analysis, gradients are crucial for comprehending the relationships between model inputs and outputs, assessing the influence of each parameter, and evaluating the robustness of model predictions. 
In optimization and inverse modelling, gradient-based methods, including gradient descent and its many variants \cite{ruder2016overview-gradient-descent}, are more efficient at finding a minimum and converge faster to them than gradient-free methods.
In Bayesian inference, gradient-based sampling strategies are better at estimating the posterior distribution than gradient-free methods \cite{neal2011mcmc}.
Therefore, accurately determining model gradients is essential for robust model understanding and effective data assimilation. 
% In other words:
% \begin{displayquote}
% \emph{A gradient serves as a compass: it tells us in which direction in the vast, open ocean of parameters we should move towards in order to increase our chances of success. }
% \end{displayquote}
In all the previous cases, the set of techniques used to compute the gradient belong to the framework of differentiable programming. 
% obtaining a gradient generally involves differentiating a computer program, which can be obtained through differentiable programming.

% Gradiens are obtained with DP
Differentiable programming (DP) refers to the set of techniques and software tools for computing gradients of a model output evaluated using a computer program with respect to model variables or parameters \cite{Shen_diff_modelling, Innes_Zygote, blondel2024elements}. 
% This set of tools are particularly useful for the estimation and computation of the gradient of the model output with respect to certain input parameter.  
% In the case of ODE-based models, DP techniques include automatic or algorithmic differentiation (AD), sensitivity equations, and adjoint methods. 
Being able to compute gradients of ODE-based models is the key to complex data assimilation models that leverage strong physical priors while offering flexibility to adapt to observations.
This is very appealing in fields such as computational physics, geophysics, and biology, to mention a few, where there is a broad literature on ODE-based models and a long tradition in numerical methods. 
Going a step further, some authors have recently suggested DP as the bridge between modern machine learning and traditional scientific models \cite{Ramsundar_Krishnamurthy_Viswanathan_2021, Shen_diff_modelling, Gelbrecht-differential-programming-Earth, rackauckas2021generalized}. 
The first goal of this work is to introduce some of the applications of this emerging technology and to motivate its incorporation for the modelling of complex systems in the natural and social sciences. 
\begin{quote}
    \textbf{Question 1. }
    \textit{What are the scientific applications of differentiable programming for dynamical systems?}
\end{quote}
We will answer this first question in Section \ref{sectopn:motivation}. 

% DP in different fields: traditional appraches and modern ML
Arguably, the notion of DP in ODE-based models, specially for inverse modelling, has a long tradition among different scientific communities.
In statistics, gradients of the likelihood function of ODE-based models enable inference on the model parameters \cite{ramsay2017dynamic}. 
In numerical analysis, sensitivities quantify how the solution of a differential equation fluctuates with respect to certain parameters. 
This is particularly useful in optimal control theory, where the goal is to find the optimal value of some control (e.g. the shape of a wing) that minimizes a given loss function \cite{Giles_Pierce_2000}. 
In recent years, there has been an increasing interest in designing machine learning workflows that include constraints in the form of differential equations.
This emerging sub-field is usually referred as physics-based machine learning \cite{Karniadakis_Kevrekidis_Lu_Perdikaris_Wang_Yang_2021, thuerey2021pbdl}.
This include methods that numerically solve differential equations, such as physics-informed neural networks \cite{PINNs_2019}, including NeuralPDEs \cite{Zubov_McCarthy_Ma_Calisto_Pagliarino_Azeglio_Bottero_Luj√°n_Sulzer_Bharambe_et} and biologically-informed neural networks \cite{Lagergren_Nardini_Baker_Simpson_Flores_2020}. 
On the other hand, there has been an increased interest in augmenting differential equation model with data-driven components, such as universal differential equations \cite{rackauckas2020universal, Dandekar_2020}, which also includes the case of neural ordinary differential equations \cite{chen_neural_2019} and neural stochastic differential equations \cite{li2020scalable}.


% SciML and large scale models 
The need of gradients is even more critical as the total number of parameters and the expressivity of the model increases, especially when considering highly non-linear processes dictated by hidden physics \cite{Karniadakis_Kevrekidis_Lu_Perdikaris_Wang_Yang_2021}.
In these cases, the \textit{curse of dimensionality} renders gradient-free optimization and sampling methods computationally intractable for most large-scale problems. 
This is certainly the case in machine learning application where neural networks are used as the default method to approximate general non-linear function. 
Furthermore, for stochastic forward models, the intractability of the likelihood function represents a major challenge for statistical inference \cite{Cranmer_Brehmer_Louppe_2020}.
The integration of automatic differentiation and, more broadly, differentiable programming, has provided new tools for resolving complex simulation-based inference problems \cite{Cranmer_Brehmer_Louppe_2020}.
The availability of second derivatives (Hessian) further helps to improve the convergence rates of these algorithms \cite{BuiThanh:2012ul}.


% Difficulties with ODE and numerical solvers
However, when working with ODE-based models, the computation of gradients is not an easy task, both regarding the mathematical framework and software implementation involved. 
Except for a small set of particular cases, most differential equations require numerical methods to approximate their solution.
This means that solutions cannot be directly differentiated and require special treatment to compute derivatives. 
Furthermore, numerical solutions introduce approximation errors. 
These errors can be propagated to the computation of the gradient, leading to incorrect gradient values. 
Alternatively, there is a broad literature on numerical methods for solving differential equations \cite{hairer-solving-1, hairer-solving-2}. 
Although each method provides different guarantees and advantages depending on the use case, this means that the tools developed to compute gradients when using a solver need to be universal enough in order to be applied to all or at least to a large set of them. 
Despite its success, integrating DP with differential equation-based models remains a significant challenge in high-performance scientific computing \cite{Naumann.2011}.
% As coined by Uwe Naumann, \textit{the automatic generation of optimal (in terms of robustness and efficiency) adjoint versions of large-scale simulation code is one of the great open challenges in the field of High-Performance Scientific Computing} \cite{Naumann.2011}.
The second goal of this article is to review different methods that exist to achieve this goal.
\begin{quote}
    \textbf{Question 2. }
    \textit{How can one efficiently compute the gradient of a function that depends on the numerical solution of a differential equation?}
\end{quote}
We will address this question from a mathematical perspective (Section \ref{section:methods}) and a computational perspective (Section \ref{sec:computational-implementation}). 

One of the challenges in implementing efficient routines to compute gradients of ODE-based models is the integration of the different methods with automatic differentiation.
The broader set of tools known as automatic or algorithmic differentiation (AD) aims to compute derivatives by sequentially applying the chain rule to the sequence of unit operations that constitute a computer program \cite{Griewank:2008kh, Naumann.2011}. 
The premise is simple: every computer program is ultimately an algorithm described by a nested concatenation of elementary algebraic operations, such as addition and multiplication, that are individually easy to differentiate and their composition is easy to differentiate by using the chain rule \cite{Giering:1998in}. 
More broadly than AD, DP tools for ODE-models further include a family of methods that compute the gradient by relying on an auxiliary set of differential equations. 
Furthermore, it is important to be aware that when using AD or any other technique we are differentiating the algorithm used to compute the numerical solution, not the numerical solution itself, which can lead to wrong results \cite{Eberhard_Bischof_1996}.


The differences between methods to compute sensitivities arise both from their mathematical formulation, numerical considerations, and their computational implementation. 
% The first provides different guarantees on the method returning the actual gradient or a good approximation thereof. 
% The second involves how theory is translated to software, and what are the data structures and algorithms used to implement it. 
Different methods have different computational complexities depending on the total number of parameters and size of the differential equation system, and these complexities are also balanced between total execution time and required memory. 
The third goal of this work, then, is to illustrate the different strengths and weaknesses of these methods, and how to use them in modern scientific software. 
\begin{quote}
    \textbf{Question 3. }
    \textit{What are the advantages and disadvantages of different differentiation methods and how can I incorporate them in my research?}
\end{quote}
We will answer this question along this paper and conclude with a series of recommendations at the end of the manuscript (Section \ref{section:recomendations}).

This paper aims at presenting a comprehensive review of methods for calculating gradients of the numerical solution of differential equations. 
It reviews differentiable programming for differential equations from three different perspectives: a domain science perspective (Section \ref{sectopn:motivation}), a mathematical perspective (Section \ref{section:methods}) and a computer science perspective (Section \ref{sec:computational-implementation}). 
By providing a common framework across all methods and applications, we hope to facilitate the development of scalable, practical, and efficient differentiable ODE-based models. 


