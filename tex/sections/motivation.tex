% \subsection{On the importance of differentiable programming}

Scientific models from many domains have often been based on mechanistic (or process-based) models, represented as differential equations, involving the use of numerical methods to solve them. 
Among many, this led to fundamental advances in the physical sciences during the last century, with the combination of complex mathematical theories and a reduced amount of observations to validate them
\cite{Wigner.1960, Rude:2018jv}. 
% From predictions of population growth (Malthus) to permitting the prediction of black holes (Einstein field equations), differential equations have provided huge scientific contributions since Newton.
The parameters and processes within process-based models have traditionally been determined independently of the model, with empirical data for model validation and comparison \cite{hartig2012}.
Independent estimation of parameters and processes rapidly becomes impossible as the number of state variables modelled increases, especially when considering highly non-linear processes. 
Inverse modelling, which consists in using observation data to recover the parameters of a model that can best explain the data, allows the bridging of this gap \cite{Wigner.1960, Rude:2018jv}. 
The process knowledge embedded in the structure of mechanistic models renders them more robust for predicting dynamics under different conditions.
Nonetheless, in the 21st century, with the unstoppable wave of data flooding all scientific domains, progress with such traditional methods has become more complex. 

Meanwhile, the field of statistics experienced a boom following the massive growth of data, signaling the era of data science and machine learning \cite{Cox:2017hv}.
With the advent of machine learning methods, it is possible to learn and capture extremely complex nonlinear patterns and information hidden in huge datasets. 
% Machine learning models can be seen as the opposite of mechanistic models, as they are flexible, data-driven and they do not necessarily respect domain-specific constraints.
Contrasting with mechanistic models, machine learning models are inherently flexible and data-driven, often operating without adhering to domain-specific constraints. 
A significant characteristic of contemporary machine learning, particularly deep learning models, is their ability to autonomously learn at multiple levels of abstraction, efficiently extracting pertinent features from large datasets \cite{LeCun2015}}.

At first sight, these two modelling philosophies may be seen as antagonistic, and this is more or less the way they have evolved in the last decades \cite{zdeborova_understanding_2020}. 
On the one hand, domain scientists have often been sceptical of adopting machine learning methods, judging them as opaque black boxes, unreliable, and not respecting domain-established knowledge \cite{Coveney:2016eb}. 
Predictions with correlative models assume that patterns contained in data can be extrapolated \cite{dormann2007}. 
However, they may fail to disentangle the respective impact of the numerous ecological processes at play and may fail to predict dramatic shifts in dynamics \cite{Barnosky2012}.
% On the other hand, the field of machine learning has mainly been developed around data-driven applications, without including any \textit{a priori} physical knowledge. 
However, there has been an increasing interest in making mechanistic models more flexible, as well as introducing domain-specific or physical constraints and interpretability in machine learning models \cite{Molnar.2020sisk,Rudin.2022,Schneider2017,rasp2018,Yazdani2020,Abarbanel2018,Carrassi2018,Bocquet2019,Gabor2015,Gharamti2017,Curtsdotter2019,Rosenbaum2019,Toms2020,Brajard2021}.
If both modelling approaches have different strengths, why not combine them and attempt to have the best of both worlds?

% A key way to achieve this is through differentiable programming, i.e., being able to compute derivatives of any computer program describing a scientific model.
% During the last decades, the backpropagation algorithm has enabled the fast growth of deep learning by efficiently computing gradients of large and complex neural networks with many parameters \cite{griewank2012invented}.
% Nowadays, the differentiation of hybrid models comprising data-driven models (e.g., neural networks, gaussian processes) with differential equations poses complex technical problems, which are only starting to be explored in recent years \cite{ma_comparison_2021}. 
% Being able to accurately estimate model parameters, ranging from a few ones in classic inversion problems to millions of them in 
% % \todo{I disagree, many geophysical inversions involve millions of parmeters; it's not specific to NNs.}
% large neural networks, opens many new possibilities. 
% Differentiable programming has the potential to revolutionize the way we approach and design scientific models and even the way we discover governing laws from data. 

\subsection{Domain-specific applications}

Arguably, the notion of differentiable programming has a long tradition in  computational physics which is founded on solving and/or inverting models based  on differential equation.
The overarching goal of such problems is to find a set of optimal model parameters that minimize an objective or cost function quantifying the misfit between observations and the simulated state.
%, subject to the constraint that the model equations be fulfilled. 
%The constrained optimization problem is transformed into an unconstrained problem by way of \emph{Lagrange multiplier method}\cite{Vadlamani.2020}, also referred to as the \emph{adjoint method}. 
% The corresponding \textit{adjoint model} computes the gradient of the objective function with respect to all inputs. 
% Gradient-based nonlinear optimization, then, enables us to invert for optimal values of the unknown or uncertain inputs.
Depending on the nature of the 
%physical 
inversion, we may distinguish between the following cases.
\begin{itemize}
    \item \textbf{Initial conditions.} Inverting for uncertain initial conditions, which, when integrated using the model, lead to an optimal match betweeen the observations and the simulated state (or diagnostics thereof); variants thereof are used for optimal forecasting.
    \item \textbf{Boundary conditions.} Inverting for uncertain surface (e.g., interface fluxes), bottom (e.g., bed properties), or lateral (e.g., open boundaries of a limited domain) boundaries, which, when used in the model, produce an optimal match of the observations; variants thereof are used in tracer or boundary (air-sea) flux inversion problems, e.g., related to the global carbon cycle.
    \item \textbf{Model parameters.} Inverting for uncertain model parameters amounts to an optimal model calibration problem. As a \textit{learning of optimal parameters from data} problem, it is the closest to machine learning applications.
\end{itemize}
Besides the use of sensitivity methods for optimization, inversion, estimation, or learning, gradients have also proven powerful tools for computing comprehensive sensitivities of quantities of interest; computing optimal perturbations (in initial or boundary conditions) that lead to maximum, non-normal amplification of specific norms of interest; and
characterizing and quantifying uncertainties by way of second derivative (Hessian) information.

In recent years the use of machine learning methods has become more popular in many scientific domains. 
Differential equations can be used to describe a large variety of dynamical systems, while data-driven regression models (e.g., neural networks, Gaussian processes, reduced-order models, basis expansions) have been demonstrated to act as universal approximators, learning any possible function if enough data is available \cite{gorban_1998}. 
This combined flexibility can be exploited by many different domain-specific problems to tailor modelling needs to both dynamics and data characteristics.

\subsubsection{Computational physics}

There is a long tradition of computational physics models based on adjoint methods and automatic differentiation pipelines. 
These include examples in 

particle physics \cite{Dorigo.2022} or quantum chemistry \cite{Arrazola.2021}

\subsubsection{Optimal design}

There is a long tradition of computational models based on sensitivity methods for optimal design and optimal control \cite{lions1971optimal, pironneau2005optimal, allaire2014shape}.
This includes applications to 
stellarator coil design \cite{McGreivy_stellarator_2021}; 
fluid dynamics \cite{Giles_Pierce_2000, mohammadi2009applied};


% \subsubsection{Computational Fluid Dynamics}
% \subsubsection{Electromagnetism}
% \subsubsection{Quantum Physics}

\subsubsection{Geosciences}

Many geoscientific phenomena are governed by global and local conservation laws (conservation of mass, momentum, energy, tracers) along with a set of empirical constitutive laws and subgrid-scale parametrization schemes. 
Together, they enable efficient description of the system's spatio-temporal evolution in terms of a set of partial differential equations (PDEs).
Example are geophysical fluid dynamics \cite{Vallis:2016kv}, describing geophysical properties of many Earth systems, such as the atmosphere, oceans, and glaciers.
In such models, calibrating model parameters is extremely challenging, due to (i) datasets being sparse in both space and time, heterogeneous, and noisy; and (ii) computational models involving high-dimensional (typically $O(10^3) - O(10^8)$) space of parameters.
Moreover, many existing mechanistic models can only partially describe observations, with many detailed physical processes being ignored or poorly parameterized. 
% The use of differentiable programming, combining PDEs and data-driven models (i.e. Universal Differential Equations) may add flexibility to mechanistic models in order to incorporate new governing laws from data (from either measurement or simulations) \cite{rackauckas2020universal}.

In the following, we sketch how differentiable programming based on  adjoint modeling has been used in different disciplines of geosciences, and how new concepts are emerging of combining inverse modeling and machine learning approaches where differentiable programming provides a key computational enabling framework, something recently coined as scientific machine learning. 
%(Note that some authors have used the notion of ``scientific machine learning'' to capture some aspects of the latter approach [REFS]).

\paragraph{Meteorology}

Numerical weather prediction (NWP) is among the most prominent fields where adjoint methods have played an important role \cite{Errico_1997}. 
The work by \cite{Talagrand.1987,Courtier.1987} introduced the use of adjoint methods to infer initial conditions that minimize the misfit between simulations and weather observations, with the value of second-derivative information also being recognized \cite{Dimet.2002}. 
This led to the development of the so-called \textit{four-dimensional variational} (4D-Var) data assimilation (DA) technique \cite{Rabier.1992,Rabier:2000uu} at the European Centre for Medium-Range Weather Forecasts (ECMWF) as one the most advanced DA approaches, and which contributed substantially to the \textit{quiet revolution} in NWP \cite{Bauer.2015}.
Related, within the framework of transient non-normal amplification or optimal excitation \cite{Farrell.1988,Farrell:1996jx}, the adjoint method has been used extensively to infer patterns in initial conditions that over time contribute to maximum uncertainty growth in forecasts \cite{Palmer:1994br,Buizza:1995in} 
and to infer the so-called \textit{Forecast Sensitivity-based Observation Impact} (FSOI) \cite{Langland:2004jo}.
Except in very few instances and for experimental purposes \cite{Giering.2006}, automatic differentiation has not been used in the development of adjoint models in NWP.
Instead, the adjoint code was derived and implemented manually.

\paragraph{Oceanography}
The recognition of the benefit of adjoint methods for use in gradient-based optimization or data assimilation in the ocean coincided roughly with that in meteorology, with some of the foundational work by \cite{Thacker:1988kp,Thacker:1988ed}. 
The first application appeared soon thereafter in the context of a basin-scale general circulation model \cite{Tziperman.1989,Tziperman:1992hg,Tziperman:1992jw}. 
An important detail is that their work already differed from the ``4D-Var'' problem of NWP in that sensitivities were computed not only with respect to initial conditions but also with respect to surface boundary conditions, i.e., air-sea fluxes of buoyancy and momentum.
Again, the role of the second-derivative, i.e., the Hessian for uncertainty quantification was readily realized \cite{Thacker:1989jf}.
Similar to the work on calculating singular vectors in the atmosphere based on tangent linear and adjoint versions of a GCM to solve a generalized eigenvalue problem, the question of El Ni\~no predictability invited model-based singular vector computations in models of the Tropical Pacific Ocean \cite{Moore:1997ci,Moore:1997fp}. Such model-based singular vectors were also later computed for optimal excitations of the North Atlantic thermohalince circulation \cite{Zanna.2010,Zanna:2011ge,Zanna:2012dw}.
Notably in the context of this review, the consortium for ``Estimating the Circulation and Climate of the Ocean'' (ECCO) \cite{Stammer.2002} set out in around 1999 to develop a parameter and state estimation framework, whereby a state-of-the-art ocean general circulation model is fit to diverse observations by way of PDE-constrained, gradient-based optimization, with the adjoint model of the GCM computing the gradient. Importantly, the adjoint model of the MIT general circulation model (MITgcm) is generated using source-to-source automatic differentiation \cite{Marotzke:1999wc,Heimbach.2005}, initially using the ``Tangent linear and Adjoint Model Compiler'' (TAMC, \cite{Giering:1998in}) and then its commercial successor ``Transformation of Algorithms in Fortran'' (TAF, \cite{Giering.2006}).
Rigorous exploitation of AD enabled the simulation framework to be significantly extended over time in terms of vastly improved model numerics \cite{Forget.2015m9i} and coupling other Earth system components, including biogeochemistry \cite{Dutkiewicz:2006gw}, sea-ice \cite{Heimbach:2010fz}, and sub-ice shelf cavities \cite{Heimbach:2012iu}.
Unlike NWP-type 4D-Var, the use of AD also enabled extension of the framework to the problem of parameter calibration (or, ``learning parameters'' in today's speak) from observations, e.g., \cite{Ferreira.2005,Stammer:2005dw,Liu:2012jd}. Arguably, this work heralded much of today's efforts in ``online'' learning of parameterization schemes, where the functional representation between the parameters and the learning data are provided by the numerical implementation of a PDF rather than by a neural network.
Th desire to make AD for Earth system models written in Fortran (to date the vast majority) has also spurned the development of alternative AD tools with powerful reverse modes, notably OpenAD \cite{Utke:2008ko} and most recently Tapenade \cite{Hascoet.2013,Gaikwad.2023,Gaikwad.2024}.
There is enormous potential to seamlessly integrate the inverse-modeling and machine-learning based approaches through the concept of differentiable programming.

\paragraph{Climate science}

The same goals that have driven the use of sensitivity information in numerical weather prediction (optimal initial conditions for forecasts) or ocean science (state and parameter estimation) apply in the world of climate modeling.
The recognition that ``good'' initial conditions (e.g., such that are closest to the real or observed system) will lead to improved forecasts on subseasonal, seasonal, interannual, or even decadal time scales has driven major community efforts (e.g., \cite{Meehl.2021}). However, there has been a lack so far in exploiting the use of gradient information to achieve optimal initialization for coupled Earth system models \cite{Frolov.2023}. 
One conceptual challenge is the presence of multiple timescales in the coupled system and the utility of gradient information beyond many synoptic time scales in the atmosphere and ocean \cite{Lea:2000gv,Lea:2002cv}.

TBD:

Coupled GCM adjoint:
\cite{Blessing.2014,Lyu.2018,Stammer:2018de}

Neural AGCM:
\cite{Kochkov.2023}

\paragraph{Transport modeling and flux inversion.}
% ... GEOS-Chem , Kaminski, ...

\paragraph{Glaciology}

Due to the difficulty of having direct observations of internal and basal rheological processes of glaciers, adjoint methods have been widely used to study them, with a first paper three decades ago \cite{macayeal1992basal}. 
Since then, the adjoint method has been applied to many different studies investigating parameter and state estimation \cite{goldberg2013parameter}, ice volume sensitivity to basal, surface and initial conditions \cite{heimbach2009greenland}, inversion of initial conditions \cite{mosbeux2016comparison} or inversion of basal friction \cite{morlighem2013inversion}.
All these studies derived the adjoint with a manual implementation. 
Additionally, the use of AD has become increasingly widespread in glaciology, paving the way for more complex modelling frameworks \cite{hascoet2018source, logan2020sicopolis}. 
Recently, differentiable programming has also facilitated the development of hybrid frameworks, combining numerical methods with data-driven models by means of universal differential equations \cite{BolibarSapienza_UDEs}. 
Alternatively, some other approaches have dropped the use of numerical solvers in favour of different flavours of physics-informed neural networks, exploring the inversion of rheological properties of glaciers \cite{wang2022discovering} and to accelerate ice thickness inversions and simulations by leveraging GPUs \cite{Jouvet_Cordonnier_Kim_Lüthi_Vieli_Aschwanden_2021, jouvet2023inversion}. 

% \paragraph{Solid Earth geophysics}
% ...

\subsubsection{Biology and ecology}


\subsubsection{Quantum physics}

Quantum optimal control has diverse applications spanning a broad spectrum of quantum systems. 
Optimal control methods have been used to optimize pulse sequences, enabling the design of high-fidelity quantum gates and the preparation of complex entangled quantum states. 
Typically, the objective is to maximize the fidelity to a target state or unitary operation, accompanied by additional constraints or costs specific to experimental demands. 
The predominant control algorithms are gradient-based optimization methods, such as gradient ascent pulse engineering (GRAPE), and rely on the computation of derivatives for solutions of the differential equations modeling the time evolution of the quantum system. 
In cases where the analytical calculation of a gradient is impractical, numerical evaluation using AD becomes a viable alternative~\cite{jirari:2009, leung:2017, abdelhafez:2019, jirari2019quantum, abdelhafez:2020, schaefer:2020, goerz:2022}. 
Specifically, AD streamlines the adjustment to diverse objectives or constraints, and its efficiency can be enhanced by employing custom derivative rules for the time propagation of quantum states as governed by solutions to the Schrödinger equation~\cite{goerz:2022}. 
Moreover, sensitivity methods for differential equations facilitate the design of feedback control schemes necessitating the differentiation of solutions to stochastic differential equations~\cite{schaefer:2021}.