\subsection{On the importance of differentiable programming}

Scientific models from many domains have often been based on mechanistic models, represented as differential equations, involving the use of numerical methods to solve them. 
Among many, this lead to fundamental advances in the physical sciences during the last century, with the combination of complex mathematical theories and a reduced amount of observations to validate them. 
Nonetheless, in the 21st century, with the unstoppable wave of data flooding all scientific domains, progress with such traditional methods has become more complex. 

Alternatively, the field of statistics experienced a boom following the massive growth of data, signaling the era of data science and machine learning.
With the advent of machine learning methods, it is possible to learn and capture extremely complex nonlinear patterns and information hidden in huge datasets. 
Machine learning models can be seen as the opposite of mechanistic models: they are flexible, data-driven and they do not necessarily respect domain-specific constraints.

At first sight, these two modelling philosophies can be seen as antagonistic, and this is more or less the way they have evolved in the last decades \cite{zdeborova_understanding_2020}. 
On the one hand, domain scientists have often struggled to adopt machine learning methods, judging them as opaque black boxes, unreliable, and not respecting domain-established knowledge. 
On the other hand, the field of machine learning has mainly been developed around data-driven applications, without including any \textit{a priori} physical knowledge. 
However, in the last years, there has been an increasing interest in making mechanistic models more flexible, as well as introducing domain-specific or physical constraints and interpretability in machine learning models. 
% If both modelling approaches have different strengths, why not combine them and attempt to have the best of both worlds?

A key way to achieve this is through differentiable programming, i.e. being able to compute derivatives of any computer program describing a scientific model.
During the last decades, the backpropagation algorithm has enabled the fast growing of deep learning by efficiently computing gradients of large and complex neural networks with many parameters \cite{griewank2012invented}.
Nowadays, the differentiation of hybrid models comprising data-driven models (e.g. neural networks, gaussian processes) with differential equations poses complex technical problems, which are only starting to be explored in recent years \cite{ma_comparison_2021}. 
Being able to accurately estimate model parameters, ranging from a few ones in classic inversion problems to millions of them in large neural networks, opens many new possibilities. 
Differentiable programming has the potential to revolutionize the way we approach and design scientific models and even the way we discover governing laws from data. 

\subsection{Domain-specific applications}

Differential equations can be used to describe a large variety of dynamical systems, while data-driven regression models (eg, neural networks, Gaussian processes, basis expansions) have been demonstrated to act as universal approximators, virtually learning any possible function if enough data is available \cite{gorban_1998}. 
This combined flexibility can be exploited by many different domain-specific problems to tailor modelling needs to both dynamics and data characteristics.

\subsubsection{Geosciences}

In geosciences, partial differential equations (PDEs) are often used to simulate fluid dynamics, describing geophysical properties of many Earth systems, such as the atmosphere, oceans, or glaciers.
In such models, calibrating model parameters is extremely challenging, due to datasets being sparse in both space and time and noisy.
Moreover, many existing mechanistic models can only partially describe observations, with many detailed physical processes being ignored or badly parametrized. 
The use of differentiable programming, combining PDEs and data-driven models (i.e. Universal Differential Equations) can add flexibility to mechanistic models in order to incorporate new governing laws from data \cite{rackauckas2020universal}.

% Add oceanography example

Glaciers act as slow fluids, flowing down-slope through the effects of gravity, and the understanding of their rheological properties (e.g. ice viscosity affecting internal deformation or sliding at the glacier-bedrock interface) is key to assessing their contribution to water resources and sea-level rise \cite{cuffey_physics_2010}. 
These rheological processes and their dependency on key large-scale environmental variables, such as the local climate or topography, are still not well understood.
The use of differentiable programming, combined with Universal Differential Equations, holds great potential to learn new empirical laws of these physical processes from large-scale remote sensing datasets. 
A recent study showed how Julia's differentiable programming capabilities can be used to optimize the parameters of a neural network, learning a function of the nonlinear ice diffusivity in a glacier ice flow PDE, to match observations \cite{bolibar_universal_2023}.
