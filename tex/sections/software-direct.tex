Direct methods are implemented independent of the structure of the differential equation and the numerical solver used to solve it. 
These include finite differences, complex step differentiation, and both forward and reverse mode AD. 

\subsubsection{Finite differences}
\label{section:software-finite-differences}

Finite differences are easy to implement manually, do not require much software support, and provide a direct way of approximating a gradient. 
In Julia, these methods are implemented in \texttt{FiniteDiff.jl} and \texttt{FiniteDifferences.jl}, which already include subroutines to determine step-sizes.
However, finite differences are less accurate and as costly as forward AD \cite{Griewank_1989} and complex-step differentiation. 
Figure \ref{fig:direct-methods} illustrates the error in computing the gradient of a simple loss function for both true analytical solution and numerical solution of a system of ODEs as a function of the stepsize $\varepsilon$ using finite differences.
Here we consider the solution of the differential equation $u'' + \omega^2 u = 0$ with initial condition $u(0)=0$ and $u'(0)=1$, which has analytical solution $u_\text{true}(t) = \sin(\omega t) / \omega$.
The numerical solution $u_\text{num}(t)$ can be founded by solving the system of ODEs
\begin{equation}
\begin{cases}
    \frac{du_1}{dt} = u_2 \,   & \qquad u_1(0) = 0 \\
    \frac{du_2}{dt} = - \omega^2 u_1 \,   & \qquad u_2(0) = 1.
    \label{eq:example-ode-direct-methods}
\end{cases}
\end{equation}
The loss function used to differentiate is given by $L(\theta) = u(10)$.
We see that finite differences are inaccurate for computing the derivative of $u_\text{true}$ with respect to $\omega$ (that is, $u'_\text{true} = \cos(\omega t) - \sin (\omega t) / \omega^2$) when the stepsize $\varepsilon$ is both too small and too large (red dashed line).
When the derivative is instead computed using the numerical solution $u_\text{num}(t)$ (red circles), the accuracy of the derivative further deteriorates due to approximation errors in the solver. 
This effect is dependent on the numerical solver tolerance.
Notice that in general we do not know the true solution of the differential equation, so the derivative of $u_\text{true}$ obtained using finite differences just serves as a lower bound of the error we expect to see when performing sensitivity analysis on top of the numerical solver. 

\begin{figure}[tb]
    \centering
    % \includegraphics[width=0.85\textwidth]{../code/finite_differences/finite_difference_derivative.pdf}
    \includegraphics[width=0.95\textwidth]{figures/direct_derivatives.pdf}
    \caption{Absolute relative error when computing the gradient of the function $u(t) = \sin (\omega t)/\omega$ with respect to $\omega$ at $t=10.0$ as a function of the stepsize $\varepsilon$. Here, $u(t)$ corresponds to the solution of the differential equation $u'' + \omega^2 u = 0$ with initial condition $u(0)=0$ and $u'(0)=1$. The blue dots correspond to the case where the relative error is computed with finite differences. The red and orange lines are for the case where $u(t)$ is numerically computed using the default Tsitouras solver \cite{Tsitouras_2011} from \texttt{OrdinaryDiffEq.jl} using different tolerances. The error when using a numerical solver is larger and it is dependent on the numerical precision of the numerical solver. $\clubsuit_\text{\code{code:figure-comparison}}$}
    \label{fig:direct-methods}
\end{figure}

\subsubsection{Automatic differentiation}

The AD algorithms described in Section \ref{section:automatic-differentiation} implement algorithmic differentiation using different methods, namely \textit{operator overloading} for AD based on dual numbers, and \textit{source code transformation} for both forward and reverse AD based on the computational graph \cite{martins2001connection}.
In this section we are going to cover how forward AD is implemented using dual numbers, postponing the implementation of AD based on computational graphs for reverse AD (Section \ref{sec:software-reverse-AD}). 

\paragraph{Forward AD based on dual numbers}
\label{section:software-Forward-AD}

Implementing forward AD using dual numbers is usually carried out using \textit{operator overloading} \cite{Neuenhofen_2018}. 
This means expanding the object associated to a numerical value to include the tangent and extending the definition of atomic algebraic functions. 
In Julia, this can be done by relying on multiple dispatch \cite{Julialang_2017}. 
The following example illustrates how to define a dual number and its associated binary addition and multiplication extensions $\clubsuit_\text{\code{code:dual-number}}$. 
\begin{jllisting}
using Base: @kwdef

@kwdef struct DualNumber{F <: AbstractFloat}
    value::F
    derivative::F
end

# Binary sum
Base.:(+)(a::DualNumber, b::DualNumber) = DualNumber(value = a.value + b.value, derivative = a.derivative + b.derivative)

# Binary product 
Base.:(*)(a::DualNumber, b::DualNumber) = DualNumber(value = a.value * b.value, derivative = a.value*b.derivative + a.derivative*b.value)
\end{jllisting}
We further overload base operations for this new type to extend the definition of standard functions by simply applying the chain rule and storing the derivative in the dual variable following Equation \eqref{eq:dual-number-function}:
\begin{jllisting}
function Base.:(sin)(a::DualNumber)
    value = sin(a.value)
    derivative = a.derivative * cos(a.value)
    return DualNumber(value=value, derivative=derivative)
end
\end{jllisting}
% With all these pieces together, we are able to propagate forward the value of a single-valued derivative through a series of algebraic operations. 

In the Julia ecosystem, \texttt{ForwardDiff.jl} implements forward mode AD with multidimensional dual numbers \cite{RevelsLubinPapamarkou2016} and the sensitivity method \texttt{ForwardDiffSensitivity} implements forward differentiation inside the numerical solver using dual numbers. 
Figure \ref{fig:direct-methods} shows the result of performing forward AD inside the numerical solver.
We can see that for this simple example forward AD performs as good as the best output of finite differences and complex step differentiation (see Section \ref{section:software-complex-step}) when optimizing by the stepsize $\varepsilon$. 
Further notice that AD is not subject to same level of numerical errors due to floating point arithmetic \cite{Griewank:2008kh}.

% \begin{jllisting}
% using OrdinaryDiffEq, ForwardDiff, Test 

% s_AD = ForwardDiff.jacobian(p -> solve(ODEProblem(f, u0, tspan, p), Tsit5()).u[end], p)
% \end{jllisting}

Implementations of forward AD using dual numbers and computational graphs require a number of operations that increases with the number of variables to differentiate, since each computed quantity is accompanied by the corresponding gradient calculations \cite{Griewank_1989}. 
This consideration also applies to the other forward methods, including finite differences and complex-step differentiation, which makes forward models prone to the curse of dimensionality with respect to the number of parameters considered.

% If well forward AD is implemented independently of the differential equation and numerical solver, the careless combination of both can lead to disastrous numerical errors. 
Although AD is always algorithmically correct, when combined with a numerical solver \textit{AD can be numerically incorrect} and result in wrong gradient calculations \cite{Eberhard_Bischof_1996}. 
% Contrary to common folk, AD is not always \textit{numerically} correct and can lead to wrong gradients. 
To illustrate this point, consider the following example of a simple system of ODEs
\begin{equation}
\begin{cases}
 \frac{du_1}{dt} = a u_1 - u_1 u_2 & \quad u_1(0) = 1  \\ 
 \frac{du_2}{dt} = - a u_2 + u_1 u_2 & \quad u_2(0) = 1
\end{cases}
\end{equation}
with $a$ the parameter with respect to which we want to differentiate. 
In the simple case of $a=1$, the solutions of the ODE are constant functions $u_1(t) \equiv u_2(t) \equiv 1$. 
As explained in Appendix \ref{appendix:dual-number-solver}, traditional adaptive stepsize solvers used for just solving ODEs are designed to control for numerical errors in the ODE solution but not in its sensitivities when coupled with an internal AD method. 
This can lead to wrong gradient calculations that propagate through the numerical solver without further warning $\clubsuit_\text{\code{code:AD-wrong}}$. 


\paragraph{Reverse AD based on computational graph}
\label{sec:software-reverse-AD}

In contrast to finite differences, forward AD, and complex-step differentiation, reverse AD is the only of this family of methods that propagates the gradient in reverse mode by relying on analytical derivatives of primitive functions, which in Julia are available via \texttt{ChainRules.jl}.
Since this requires the evaluation intermediate variables, reverse AD requires a more delicate protocol of how to store intermediate variables in memory and make them accessible during the backwards pass. 

Reverse AD can be implemented via \textit{pullback} functions \cite{Innes_2018}, a method also known as \textit{continuation-passing style} \cite{Wang_Zheng_Decker_Wu_Essertel_Rompf_2019}.
In the backward step, it executes a series of function calls, one for each elementary operation.
If one of the nodes in the graph $w$ is the output of an operation involving the nodes $v_1, \ldots, v_m$, where $v_i \rightarrow w$ are all nodes in the graph, then the pullback $\bar v_1, \ldots, \bar v_m = \mathcal B_w(\bar w)$ is a function that accepts gradients with respect to $w$ (defined as $\bar w$) and returns gradients with respect to each $v_i$ (defined as $\bar v_i$) by applying the chain rule. 
Consider the example of the multiplicative operation $w = v_1 \times v_2$. Then
\begin{equation}
 \bar v_1, \bar v_2 
 \,=\,
 v_2 \times \bar w , 
 v_1 \times \bar w 
 \,=\,
 \mathcal{B}_w (\bar w),
\end{equation}
which is equivalent to using the chain rule as
\begin{equation}
 \frac{\partial \ell}{\partial v_1} = \frac{\partial}{\partial v_1}(v_1 \times v_2) \frac{\partial \ell}{\partial w}.
\end{equation}
% There are many libraries that implement reverse AD acroos programming languages \cite{Baydin_Pearlmutter_Radul_Siskind_2015}.
% In Julia, the libraries \texttt{ReverseDiff.jl} and \texttt{Zygote.jl} use pullbacks to compute gradients \cite{Innes_2018}. 
% When gradients are being computed with less than $\sim 100$ parameters, the former is faster (see documentation).

% \subsubsection{Further remarks}

A crucial distinction between AD implementations based on computational graphs is between \textit{static} and \textit{dynamic} methods\cite{Baydin_Pearlmutter_Radul_Siskind_2015}. 
In the case of a static implementations, the computational graph is constructed before any code is executed, which are encoded and optimized for performance within the graph language. 
For static structures such as neural networks, this is ideal \cite{abadi-tensorflow}. 
However, two major drawbacks of static methods are composability with existing code, including support of custom types, and adaptive control flow, which is a common feature of numerical solvers. 
These issues are addressed in reverse AD implementations using \textit{tracing}, where the program structure is transformed into a list of pullback functions that built the graph dynamically at runtime. 
Popular libraries in this category are \texttt{Tracker.jl} and \texttt{ReverseDiff.jl}. A major drawback of tracing systems is that the pullbacks are constructed with respect to the control flow of the input value and thus do not necessarily generalize to other inputs. This means that the pullback must be reconstructed for each now forward pass, limiting the reuse of computational optimizations or inducing higher overhead. Source-to-source AD systems can achieve higher performance by giving a static derivative representation to  arbitrary control flow structure, thus allowing for the construction and optimization of pullbacks independent of the input value. 
These include \texttt{Zygote.jl}\cite{Innes_Zygote}, \texttt{Enzyme.jl}\cite{moses_Enzyme}, and \texttt{Difractor.jl}.
The existence of multiple AD packages lead to the development of \texttt{AbstractDifferentiation.jl} which allows to combine different methods \cite{Schäfer_Tarek_White_Rackauckas_2021}. 


% Since perturbation confusion also affects reverse mode AD, maybe is better to cover that here.

\paragraph{Discrete checkpointing}
\label{section:checkpointing}

In opposition to forward methods, all reverse methods, including backpropagation and adjoint methods, require to access the value of intermediate variables during the propagation of the gradient. 
For a numerical solver, the number of memory required to accomplish this can be very large, involving a total of at least $\mathcal O(nk)$ terms, with $k$ the number of steps of the numerical solver. 
Checkpointing is a technique that can be used for all the backwards methods that avoids storing all the intermediate states by balancing  storing and recomputation to recover the required state exactly \cite{Griewank:2008kh}.
This is achieved by saving intermediate states of the solution in the forward pass and recalculating the solution between intermediate states in the backwards mode \cite{Griewank:2008kh, Checkpoiting_2023}. 

Talk about MALI \cite{Zhuang_Dvornek_Tatikonda_Duncan_2021}. 

% Read chapter in Griewank!!!
% Margossian has a nice discussion on this topic

% To expand

\subsubsection{Complex step differentiation}
\label{section:software-complex-step}
% For credit: The figure in the left coicindes with the one in  \cite{fike2013multi}
% Read more carefully Martins et. al. (2001) for the computational aspects of complex step.

Modern software already have support for complex number arithmetic, making complex step differentiation very easy to implement.
In Julia, complex analysis arithmetic can be easily carried inside the numerical solver.
The following example shows how to extend the numerical solver used to solve the ODE in Equation \eqref{eq:example-ode-direct-methods} to support complex numbers $\clubsuit_\text{\code{code:complex-step}}$.
\begin{jllisting}
function dyn!(du::Array{Complex{Float64}}, u::Array{Complex{Float64}}, p, t)
    ω = p[1]
    du[1] = u[2]
    du[2] = - ω^2 * u[1]
end

tspan = [0.0, 10.0]
du = Array{Complex{Float64}}([0.0])
u0 = Array{Complex{Float64}}([0.0, 1.0])

function complexstep_differentiation(f::Function, p::Float64, ε::Float64)
    p_complex = p + ε * im
    return imag(f(p_complex)) / ε
end

complexstep_differentiation(x -> solve(ODEProblem(dyn!, u0, tspan, [x]), Tsit5()).u[end][1], 20., 1e-3)
\end{jllisting}

Figure \ref{fig:direct-methods} further shows the result of performing complex step differentiation using the same example as in Section \ref{section:software-finite-differences}.
We can see from both exact and numerical solution that complex-step differentiation does not suffer from small values of $\varepsilon$, meaning that $\varepsilon$ can be chosen arbitrarily small \cite{martins2001connection} as long as it does not reach the underflow threshold \cite{Goldberg_1991_floatingpoint}. 
Notice that for large values of the stepsize $\varepsilon$ complex step differentiation gives similar results than finite differences, while for small values of $\varepsilon$ the performance of complex step differentiation is slightly worse than forward AD. 
This result emphasize the observation made in Section \ref{section:comparison-discrete-adjoint-AD}, complex step differentiation has many aspects in common with finite differences and AD based on dual numbers. 

% This resemblance between the methods makes them susceptible to the same advantages and disadvantages: easiness of implementation with operator overloading; and inefficient scaling with respect to the number of variables to differentiate. 
However, the difference between the methods also makes the complex step differentiation sometimes more efficient than both finite differences and AD \cite{Lantoine_Russell_Dargent_2012}, an effect that can be counterbalanced by the number of extra unnecessary operations that complex arithmetic requires (see last column in Figure \ref{fig:complex-step-AD}) \cite{Martins_Sturdza_Alonso_2003_complex_differentiation}.
