
Sensitivity methods based on numerical solvers tend to be better adapted to the structure and properties of the underlying ODE (stiffness, stability, accuracy) but are also more difficult to implement.  
This difficulty arises from the fact that the sensitivity method needs to deal with some numerical and computational considerations, including how to handle matrix/Jacobian-vector products; numerical stability of the forward/backward solver; and memory-time tradeoff. 
These factors are further exacerbated by the number of ODEs and parameters in the model. 
% While explicit methods can be preferable for non-stiff problems, Rosenblock methods can be 
Just a few modern scientific software have the capabilities of handling ODE solvers and computing their sensitivities at the same time. 
These include 
\texttt{CVODES} within \texttt{SUNDIALS} in C \cite{serban2005cvodes, SUNDIALS-hindmarsh2005sundials}; 
\texttt{ODESSA} \cite{ODESSA} and \texttt{FATODE} (discrete adjoints) \cite{FATODE2014} both in Fortram; 
\texttt{SciMLSensitivity.jl} in Julia \cite{rackauckas2020universal}; 
\texttt{Dolfin-adjoint} based on the \texttt{FEniCS} Project \cite{dolfin2013, dolfin2018};
\texttt{DENSERKS} in Fortram\cite{alexe2007denserks}; 
and \texttt{Diffrax} in Python\cite{kidger2021on}. 

It is important to remark that the underlying machinery of all solvers relies on solvers for linear systems of equations, which can be solved in dense, band (sparse), and Krylov mode. 
% This implies that methods based on numerical solvers are, in principle, more difficult to implement but also more efficient in computing gradients for complex differential equations. 
Another important consideration is that all these methods have subroutines to compute the VJPs involved in the sensitivity and adjoint equations. 
This calculation is carried out by another sensitivity method (finite differences, AD) which plays a central role when analyzing the accuracy and stability of the adjoint method. 

\subsubsection{Sensitivity equation}
\label{section:computing-sensitivity-equations}

For systems of equations with few number of parameters, this method is useful since the system of $n(p+1)$ equations composed by Equations \eqref{eq:original_ODE} and \eqref{eq:sensitivity_equations} can be solved using the same precision for both solution and sensitivity numerical evaluation. 
Furthermore, this is a forward method and it does not required saving the solution in memory. 
The simplicity of the sensitivity method makes it available in most software for sensitivity analysis, including ... 

However, for stiff systems of ODEs the use of the sensitivity equations is unfeasible \cite{kim_stiff_2021}.
For systems of stiff-ODEs, the cost of numerically stable solvers is cubic in the number of ODEs\cite{hairer-solving-2}, making the complexity of the sensitivity method $\mathcal{O}(n^3p^3)$. 
This complexity makes this method useless for models with many ODEs and/or parameters. 

% Implemented as CSA in Julia

\paragraph{Computing VJPs inside the solver}
\label{section:computing-vjp-inside-solver}

All the solver-based methods has to faced the challenge of how to compute large VJPs. 
In the case of the sensitivity equation, this correspond to the row/column terms in $\frac{\partial f}{\partial u} s $ in Equation \eqref{eq:sensitivity_equations}.
For the adjoint equations, although an efficient trick has been used to remove the most computationally expensive VJP, we still need to evaluate the term $\lambda^T \frac{\partial G}{\partial \theta}$ for the discrete adjoint method in Equation \eqref{eq:def_adjoint}, and $\lambda^T \frac{\partial f}{\partial \theta}$ for the continuous adjoint method in Equation \eqref{eq:casa-final-loss-gradient}. 
% Mention why these are different?
Therefore, the choice of the specific algorithm to compute VJPs can have significant impact in the overall performance. 

In SUNDIALS, the VJPs involved in the sensitivity and adjoint method are handled using finite differences unless specified by the user \cite{SUNDIALS-hindmarsh2005sundials}.
In FATODE, these can be computed with finite differences, AD or provides by the user.
In the Julia ecosystem, different AD packages are available for this task (see Section \ref{sec:software-reverse-AD}), including \texttt{ForwardDiff.jl}, \texttt{ReverseDiff.jl}, \texttt{Zygote.jl}\cite{Innes_Zygote}, \texttt{Enzyme.jl}\cite{moses_Enzyme}, \texttt{Tracker.jl}.

\subsubsection{Adjoint methods}
\label{section:computing-adjoints}

% Distinctio between discrete and continuous
% Discrete are the exact gradient of the computer program, continuous are not. 
% There is no flexibilty in discrete adjoitns, but there is in continuous
% Human effort required to compute discrete adjoints is large \cite{FATODE2014}

For complex and large systems, direct methods for computing the gradient on top of the numerical solver can be memory expensive since the large number of function evaluations required by the solver and the later store of the intermediate states. 
For these cases, the adjoint-based method allows us to compute the gradients of a loss function by instead computing the adjoint that serves as a bridge between the solution of the ODE and the final sensitivity. 
If well the adjoint method offers considerate advantages when working with complex systems, since we are dealing with a new differential equation special care needs to be taken with respect to numerical efficiency and stability
Some works have shown that continuous adjoints can lead to unstable sensitivities \cite{Jensen_Nakshatrala_Tortorelli_2014}.
% Implicit forward schemes can give rise to explicit backwards schemes, leading to unstable solutions for the gradient. \todo{not clear}

...

\paragraph{Solving the backwards mode and checkpointing}

The bottleneck of this method is the calculation of the adjoint, since in order to solve the adjoint equation we need to know $u(t)$ at any given time. 
Effectively, notice that the adjoint equation involves the terms $f(u, \theta, t)$ and $\frac{\partial h}{\partial u}$ which are both functions of $u(t)$. 
There are different ways of addressing the evaluation of $u(t)$ during the backwards step.
\begin{enumerate}[label=(\roman*)]
    \item \textbf{Dense Output.} During the forward model, we can store in memory all the intermediate states of the numerical solution. 
    This leads to heavy-memory expensive algorithms. 
    However, in opposition to discrete methods, solving for the adjoint in reverse mode requires to be able to evaluate the solution $u(t)$ at any given time $t$. 
    This can be done using dense output formulas, for example by adding extra stages to the Runge-Kutta scheme (Equation ...) that allows to define a continuous interpolation, a method known as continuous Runge-Kutta \cite{hairer-solving-2, Alexe_Sandu_2009}. 
    \item \textbf{Backsolve.} Solve again the original ODE together with the adjoint as the solution of the reversed augmented system \cite{chen_neural_2019}
    \begin{equation}
    \frac{d}{dt}
    \begin{bmatrix}
       u \\
       \lambda \\
       \frac{dL}{d\theta}
    \end{bmatrix}
    = 
    \begin{bmatrix}
       -f \\
       - \frac{\partial f}{\partial u}^T \lambda - \frac{\partial h}{\partial u}^T \\
       - \lambda^T \frac{\partial f}{\partial \theta} - \frac{\partial h}{\partial \theta}
    \end{bmatrix}
    % = 
    % - [ 1, \lambda^T, \lambda^T ]
    % \begin{bmatrix}
    %    f & \frac{\partial f}{\partial u} & \frac{\partial f}{\partial \theta} \\
    %    0 & 0 & 0 \\
    %    0 & 0 & 0
    % \end{bmatrix},
    \qquad 
    \begin{bmatrix}
       u \\
       \lambda \\
       \frac{dL}{d\theta}
    \end{bmatrix}(t_1)
    = 
    \begin{bmatrix}
       u(t_1) \\
       \frac{\partial L}{\partial u(t_1)} \\
       \lambda(t_0)^T s(t_0)
    \end{bmatrix}.
    \end{equation}
    However, computing the ODE backwards can be unstable and lead to large numerical errors \cite{kim_stiff_2021, Zhuang_2020}. 
    \item \textbf{Checkpointing. } Also known as windowing, checkpointing is a technique that sets a trade-off between memory and time by saving intermediate states of the solution in the forward pass and recalculating the solution between intermediate states in the backwards mode \cite{Griewank:2008kh, Checkpoiting_2023}. 
    This is implemented in \texttt{Checkpointing.jl} \cite{Checkpoiting_2023}.
\end{enumerate} 

One way of solving this system of equations that ensures stability is by using implicit methods. 
However, this requires cubic time in the total number of ordinary differential equations, leading to a total complexity of $\mathcal O((n+p)^3)$ for the adjoint method.
Two alternatives are proposed in \cite{kim_stiff_2021}, the first referred to as \textit{Quadrature Adjoint} produces a high order interpolation of the solution $u(t)$ as we move forward, then solve for $\lambda$ backwards using an implicit solver and finally integrating $\frac{dL}{d\theta}$ in a forward step.
This reduces the complexity to $\mathcal O (n^3 + p)$, where the cubic cost in the number of ODEs comes from the fact that we still need to solve the original stiff differential equation in the forward step. 
A second but similar approach is to use an implicit-explicit (IMEX) solver, where we use the implicit part for the original equation and the explicit for the adjoint. 
This method also has a complexity of $\mathcal O (n^3 + p)$.

When using Runge-Kutta methods, it is important that both the forward and backwards solver use the same Runge-Kutta coefficients (Equation \eqref{eq:Runge-Kutta-scheme}) in order to both have the same level of accuracy \cite{Alexe_Sandu_2009}.

\paragraph{Solving the quadrature}

Another computational consideration is how the integral in Equation \eqref{eq:casa-final-loss-gradient} is numerically evaluated. 
Some methods save computation by noticing that the last step in the continuous adjoint method of evaluating $\frac{dL}{d\theta}$ is an integral instead of an ODE, and then can be evaluated as such without the need to include it in the tolerance calculation inside the numerical solver \cite{that-is-not-an-ode}.
Numerical integration, also known as quadrature integration, consists in approximating integrals by finite sums of the form
Numerical solutions of the integral 
\begin{equation}
    \int_{t_0}^{t_1} 
    F(t) dt
    \approx
    \sum_{i=1}^K \omega_i \, F(\tau_i),
\end{equation}
where the evaluation of the function occurs in certain knots $t_0 \leq \tau_1 < \ldots < \tau_K \leq t_1$, and $\omega_i$ are weights. 
Weights and knots are obtained in order to maximize the order in which polynomials are exactly integrated \cite{stoer2002-numerical}. 

Different quadrature methods are based on different choices of the knots and associated weights.
Between these methods, the Gaussian quadrature is the faster method to evaluate one-dimensional integrals \cite{Norcliffe_gaussquadrature_2023}.

\paragraph{Comparison between discrete and continuous adjoint}