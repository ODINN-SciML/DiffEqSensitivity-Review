
Sensitivity methods based on numerical solvers tend to be better adapted to the structure and properties of the underlying ODE (stiffness, stability, accuracy) but are also more difficult to implement.  
This difficulty arises from the fact that the sensitivity method needs to deal with some numerical and computational considerations, including i) how to handle matrix/Jacobian-vector products; ii) numerical stability of the forward/reverse solver; and iii) memory-time tradeoff. 
These factors are further exacerbated by the number of ODEs and parameters in the model. 
% While explicit methods can be preferable for non-stiff problems, Rosenblock methods can be 
Just a few modern scientific software have the capabilities of solving forward ODE and computing their sensitivities at the same time. 
These include 
\texttt{CVODES} within \texttt{SUNDIALS} in C \cite{serban2005cvodes, SUNDIALS-hindmarsh2005sundials}; 
\texttt{ODESSA} \cite{ODESSA} and \texttt{FATODE} (discrete adjoints) \cite{FATODE2014} both in Fortram; 
\texttt{SciMLSensitivity.jl} in Julia \cite{rackauckas2020universal}; 
\texttt{Dolfin-adjoint} based on the \texttt{FEniCS} Project \cite{dolfin2013, dolfin2018};
\texttt{DENSERKS} in Fortram\cite{alexe2007denserks}; 
\texttt{DASPKADJOINT} \cite{Cao_Li_Petzold_2002};
and \texttt{Diffrax} in Python\cite{kidger2021on}. 

It is important to remark that the underlying machinery of all solvers relies on solvers for linear systems of equations, which can be solved in dense, band (sparse), and Krylov mode. 
% This implies that methods based on numerical solvers are, in principle, more difficult to implement but also more efficient in computing gradients for complex differential equations. 
Another important consideration is that all these methods have subroutines to compute the VJPs involved in the sensitivity and adjoint equations. 
This calculation is carried out by another sensitivity method, usually finite differences or AD, which plays a central role when analyzing the accuracy and stability of the adjoint method. 

\subsubsection{Sensitivity equation}
\label{section:computing-sensitivity-equations}

For systems of equations with few number of parameters, this method is useful since the system of $n(p+1)$ equations composed by Equations \eqref{eq:original_ODE} and \eqref{eq:sensitivity_equations} can be solved using the same precision for both solution and sensitivity numerical evaluation. 
Furthermore, it does not required saving the solution in memory. 
The following example illustrates how Equation \eqref{eq:example-ode-direct-methods} and the sensitivity equation can be solved simultaneously using the simple explicit Euler method $\clubsuit_\text{\code{code:sensitivity-equation}}$: 
\begin{jllisting}
ω = 0.2
p = [ω]
u0 = [0.0, 1.0]
tspan = [0.0, 10.0]

# Dynamics
function f(u, p, t)
    du₁ = u[2]
    du₂ = - p[1]^2 * u[1]
    return [du₁, du₂]
end

# Jacobian ∂f/∂p
function ∂f∂p(u, p, t)
    Jac = zeros(length(u), length(p))
    Jac[2,1] = -2*p[1]*u[1]
    return Jac
end

# Jacobian ∂f/∂u
function ∂f∂u(u, p, t)
    Jac = zeros(length(u), length(u))
    Jac[1,2] = 1
    Jac[2,1] = -p[1]^2
    return Jac
end

# Explicit Euler method
function sensitivityequation(u0, tspan, p, dt)
    u = u0
    sensitivity = zeros(length(u), length(p))
    for ti in tspan[1]:dt:tspan[2]
        sensitivity += dt * (∂f∂u(u, p, ti) * sensitivity + ∂f∂p(u, p, ti))
        u += dt * f(u, p, ti) 
    end
    return u, sensitivity   
end

u, s = sensitivityequation(u0, tspan , p, 0.001)
\end{jllisting}
The simplicity of the sensitivity method makes it available in most software for sensitivity analysis. 
In Julia, the \texttt{ForwardSensitivity} method implements continuous sensitivity analysis, which performs forward AD on the solver via \texttt{ForwardDiff.jl} (see Section \ref{section:forwardAD-sensitivity}).
The same result can be achieve by:
% \begin{jllisting}
% using OrdinaryDiffEq, ForwardDiff, Test 

% s_AD = ForwardDiff.jacobian(p -> solve(ODEProblem(f, u0, tspan, p), Tsit5()).u[end], p)
% @test s_AD ≈ s rtol=0.01
% \end{jllisting}
\begin{jllisting}
using Zygote, SciMLSensitivity

function f!(du, u, p, t)
    du[1] = u[2]
    du[2] = - p[1]^2 * u[1]
end

s = Zygote.jacobian(p->solve(ODEProblem(f!, u0, tspan, p), Tsit5(), u0=u0, p=p, sensealg=ForwardSensitivity())[end], p)
\end{jllisting}
Notice that in the last example we needed to re-defined the out-of-place function \texttt{f} to the in-place version \texttt{f!}.

For stiff systems of ODEs the use of the sensitivity equations is unfeasible \cite{kim_stiff_2021}.
This is because stiff ODEs require the use of stable solvers with cubic cost with respect to the number of ODEs\cite{hairer-solving-2}, making the total complexity of the sensitivity method $\mathcal{O}(n^3p^3)$. 
This complexity makes this method useless for models with many ODEs and/or parameters. 


\paragraph{Computing VJPs inside the solver}
\label{section:computing-vjp-inside-solver}

All solver-based methods require the computation of VJPs. 
In the case of the sensitivity equation, this correspond to the row/column terms in $\frac{\partial f}{\partial u} s $ in Equation \eqref{eq:sensitivity_equations}.
For the adjoint equations, although an efficient trick has been used to remove the computationally expensive VJP, we still need to evaluate the term $\lambda^T \frac{\partial G}{\partial \theta}$ for the discrete adjoint method in Equation \eqref{eq:def_adjoint}, and $\lambda^T \frac{\partial f}{\partial \theta}$ for the continuous adjoint method in Equation \eqref{eq:casa-final-loss-gradient}. 
% Mention why these are different?
Therefore, the choice of the algorithm to compute VJPs can have a significant impact in the overall performance. 

In \texttt{SUNDIALS}, the VJPs involved in the sensitivity and adjoint method are handled using finite differences unless specified by the user \cite{SUNDIALS-hindmarsh2005sundials}.
In \texttt{FATODE}, these can be computed with finite differences, AD, or it can be provided by the user.
In the Julia ecosystem, different AD packages are available for this task (see Section \ref{sec:software-reverse-AD}), including \texttt{ForwardDiff.jl}, \texttt{ReverseDiff.jl}, \texttt{Zygote.jl}\cite{Innes_Zygote}, \texttt{Enzyme.jl}\cite{moses_Enzyme}, \texttt{Tracker.jl}.
These will compute the VJPs using some form of AD, which will result in correct calculations but potentially sub-optimal code. 
In Julia, the options \texttt{autodiff} and \texttt{autojacvec} allow to customized if VJPs are computed using AD or finite differences.  
% For these cases, customized VJPs function can be passed to the sensitivity methods using the \texttt{autojacvec}. 

\subsubsection{Adjoint methods}
\label{section:computing-adjoints}

% Distinctio between discrete and continuous
% Discrete are the exact gradient of the computer program, continuous are not. 
% There is no flexibilty in discrete adjoitns, but there is in continuous
% Human effort required to compute discrete adjoints is large \cite{FATODE2014}

For complex and large systems, direct methods for computing the gradient on top of the numerical solver can be memory expensive due to the large number of function evaluations required by the solver and the later store of the intermediate states. 
For these cases, adjoint-based methods allows us to compute the gradients of a loss function by instead computing the adjoint that serves as a bridge between the solution of the ODE and the final gradient. 
If well the adjoint method offers considerate advantages when working with complex systems, since we are dealing with a new differential equation special care needs to be taken with respect to numerical efficiency and stability.

\paragraph{Discrete adjoint method}

In order to illustrate how the discrete adjoint method works, the following example shows how to manually solve for the gradient of the solution of \eqref{eq:example-ode-direct-methods} using an explicit Euler method. 
\begin{jllisting}
function discrete_adjoint_method(u0, tspan, p, dt)
    u = u0
    times = tspan[1]:dt:tspan[2]

    λ = [1.0, 0.0]
    ∂L∂θ = zeros(length(p))
    u_store = [u]

    # Forward pass to compute solution
    for t in times[1:end-1]
        u += dt * f(u, p, t)
        push!(u_store, u)
    end

    # Reverse pass to compute adjoint
    for (i, t) in enumerate(reverse(times)[2:end])
        u_memory = u_store[end-i+1]
        λ += dt * ∂f∂u(u_memory, p, t)' * λ
        ∂L∂θ += dt * λ' * ∂f∂p(u_memory, p, t)
    end

    return ∂L∂θ
end

∂L∂θ = discrete_adjoint_method(u0, tspan, p, 0.001) 
\end{jllisting}
In this case, the full solution in the forward pass is stored in memory and used to compute the adjoint and integrate the loss function during the reverse pass. 
As in the case of reverse AD, checkpointing can be used here. 

The previous example shows a manual implementation of the adjoint method. 
However, as we discuss in Section \ref{section:comparison-discrete-adjoint-AD}, the discrete adjoint method can be directly been implemented using reverse AD. 
In the Julia SciML ecosystem, \texttt{ReverseDiffAdjoint} performs reverse AD on the numerical solver via \texttt{ReverseDiff.jl}; \texttt{ZygoteAdjoint} via \texttt{Zygote.jl}; and \texttt{TrackerAdjoint} via \texttt{Tracker.jl}. 
In all these cases, a custom pullback function needs to be specified that specifies how VJPs are computed thought the numerical solver \cite{rackauckas2021generalized}.

% Mention that in many cases this is implemented manually

\paragraph{Continuous adjoint method}

\begin{table}[bt]
\centering
\setlength{\tabcolsep}{10pt} % Default value: 6pt
\renewcommand{\arraystretch}{1.5} % Default value: 1
% \footnotesize
% \scriptsize
\begin{tabular}{ c |c c c c|} 
 \cline{2-5}
 &\textbf{Method} & \textbf{Stability} & \textbf{Stiff Performance} & \textbf{Memory} 
 \\ [0.5ex] 
 \cline{2-5}
 \hline
 \multirow{3}*{\rotatebox{90}{\textbf{Discrete}}}  
 &ReverseDiffAdjoint & Best & $\mathcal O (n^3 + p)$ & High \\
 &ZygoteAdjoint & Best & $\mathcal O (n^3 + p)$ & High \\
 &TrackerAdjoint & Best & $\mathcal O (n^3 + p)$ & High
 \\ [0.5ex] 
 \hline\hline
 \multirow{7}*{\rotatebox{90}{\textbf{Continuous}}} 
 & Sensitivity equation & Good & $\mathcal O (n^3p^3)$ & $\mathcal O(1)$ \\
 \cline{2-5}
 &Backsolve adjoint & Poor & $\mathcal O ((n+p)^3)$ & $\mathcal O(1)$ \\ 
 &Backsolve adjoint$^\blacktriangleleft$ & Medium & $\mathcal O ((n+p)^3)$ & $\mathcal O (K)$ \\
 &Interpolating adjoint & Good & $\mathcal O ((n+p)^3)$ & High \\ 
 &Interpolating adjoint$^\blacktriangleleft$ & Good & $\mathcal O ((n+p)^3)$ & $\mathcal O (K)$ \\
 &Quadrature adjoint & Good & $\mathcal O (n^3 + p)$ & High \\
 &Gauss adjoint & Good & $\mathcal O (n^3 + p)$ & High \\
 % name & .. & .. & .. \\
 \hline
\end{tabular}
% \vspace
\caption{Comparison in performance and cost of solver-based methods. Methods that are being checkpointed are indicated with the symbol $\blacktriangleleft$, with $K$ the total number of checkpoints.}
\label{table:adjoint}
\end{table}

The continuous adjoint methods offers a series of advantages over the discrete method and the rest of the forward methods previously discussed. 
Just as the discrete adjoint methods and backpropagation, the bottleneck is how to solve for the adjoint $\lambda(t)$ due to its dependency with VJPs involving the state $u(t)$.
Effectively, notice that Equation \eqref{eq:casa-adjoint-equation} involves the terms $f(u, \theta, t)$ and $\frac{\partial h}{\partial u}$, which are both functions of $u(t)$. 
In opposition to the discrete adjoint methods, notice that here the full continuous trajectory $u(t)$ is needed, instead of its discrete pointwise evaluation. 
There are two solutions for addressing the evaluation of $u(t)$ during the backwards step.
\begin{enumerate}[label=(\roman*)]
    \item \textbf{Interpolation.} During the forward model, we can store in memory intermediate states of the numerical solution allowing the dense evaluation of the numerical solution at any given time. 
    % This leads to heavy-memory expensive algorithms. 
    % However, in opposition to discrete methods, solving for the adjoint in reverse mode requires to be able to evaluate the solution $u(t)$ at any given time $t$. 
    This can be done using dense output formulas, for example by adding extra stages to the Runge-Kutta scheme (Equation \eqref{eq:Runge-Kutta-scheme}) that allows to define a continuous interpolation, a method known as continuous Runge-Kutta \cite{hairer-solving-2, Alexe_Sandu_2009}. 
    % When using checkpointing, intermediate variables are saved and the interpolation between them is re-computed on demand. 
    \item \textbf{Backsolve.} Solve again the original system of ODEs together with the adjoint as the solution of the reversed augmented system \cite{chen_neural_2019}
    \begin{equation}
    \frac{d}{dt}
    \begin{bmatrix}
       u \\
       \lambda \\
       \frac{dL}{d\theta}
    \end{bmatrix}
    = 
    \begin{bmatrix}
       -f \\
       - \frac{\partial f}{\partial u}^T \lambda - \frac{\partial h}{\partial u}^T \\
       - \lambda^T \frac{\partial f}{\partial \theta} - \frac{\partial h}{\partial \theta}
    \end{bmatrix}
    % = 
    % - [ 1, \lambda^T, \lambda^T ]
    % \begin{bmatrix}
    %    f & \frac{\partial f}{\partial u} & \frac{\partial f}{\partial \theta} \\
    %    0 & 0 & 0 \\
    %    0 & 0 & 0
    % \end{bmatrix},
    \qquad 
    \begin{bmatrix}
       u \\
       \lambda \\
       \frac{dL}{d\theta}
    \end{bmatrix}(t_1)
    = 
    \begin{bmatrix}
       u(t_1) \\
       \frac{\partial L}{\partial u(t_1)} \\
       \lambda(t_0)^T s(t_0)
    \end{bmatrix}.
    \end{equation}
    An important problem with this approach is that computing the ODE backwards $\frac{du}{dt} = - f(u,\theta, t)$ can be unstable and lead to large numerical errors \cite{kim_stiff_2021, Zhuang_2020}. 
    One way of solving this system of equations that ensures stability is by using implicit methods. 
    However, this requires cubic time in the total number of ordinary differential equations, leading to a total complexity of $\mathcal O((n+p)^3)$ for the adjoint method.
\end{enumerate} 
Both interpolating and backsolve adjoint methods can be implemented along with a checkpointing scheme. 
This is implemented in \texttt{Checkpointing.jl} \cite{Checkpoiting_2023}.

When dealing with stiff differential equations, special considerations need to be taken into account.
Two alternatives are proposed in \cite{kim_stiff_2021}, the first referred to as \textit{Quadrature Adjoint} produces a high order interpolation of the solution $u(t)$ as we move forward, then solve for $\lambda$ backwards using an implicit solver and finally integrating $\frac{dL}{d\theta}$ in a forward step.
This reduces the complexity to $\mathcal O (n^3 + p)$, where the cubic cost in the number of ODEs comes from the fact that we still need to solve the original stiff differential equation in the forward step. 
A second similar approach is to use an implicit-explicit (IMEX) solver, where we use the implicit part for the original equation and the explicit for the adjoint. 
This method also has a complexity of $\mathcal O (n^3 + p)$.



\paragraph{Solving the quadrature}

Another computational consideration is how the integral in Equation \eqref{eq:casa-final-loss-gradient} is numerically evaluated. 
Some methods save computation by noticing that the last step in the continuous adjoint method for evaluating $\frac{dL}{d\theta}$ is an integral instead of an ODE, and then it can be computed without the need to include it inside the numerical solver \cite{that-is-not-an-ode}.
Numerical integration, also known as quadrature integration, consists in approximating integrals by finite sums of the form
\begin{equation}
    \int_{t_0}^{t_1} 
    F(t) dt
    \approx
    \sum_{i=1}^K \omega_i \, F(\tau_i),
\end{equation}
where the evaluation of the function occurs in certain knots $t_0 \leq \tau_1 < \ldots < \tau_K \leq t_1$, and $\omega_i$ are weights. 
Weights and knots are obtained in order to maximize the order in which polynomials are exactly integrated \cite{stoer2002-numerical}. 
Different quadrature methods are based on different choices of the knots and associated weights.
Between these methods, the Gaussian quadrature is the faster method to evaluate one-dimensional integrals \cite{Norcliffe_gaussquadrature_2023}, which gives the name to the Gaussian adjoint method in Table \ref{table:adjoint}.

