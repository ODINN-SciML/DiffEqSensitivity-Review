
Sensitivity methods based on numerical solvers tend to be better adapted to the structure and properties of the underlying ODE (stiffness, stability, accuracy) but are also more difficult to implement.  
This difficulty arises from the fact that the sensitivity method needs to deal with some numerical and computational considerations, including i) how to handle matrix/Jacobian-vector products; ii) numerical stability of the forward/reverse solver; and iii) memory-time tradeoff. 
These factors are further exacerbated by the number of ODEs and parameters in the model. 
% While explicit methods can be preferable for non-stiff problems, Rosenblock methods can be 
Just a few modern scientific software have the capabilities of solving forward ODE and computing their sensitivities at the same time. 
These include 
\texttt{CVODES} within \texttt{SUNDIALS} in C \cite{serban2005cvodes, SUNDIALS-hindmarsh2005sundials}; 
\texttt{ODESSA} \cite{ODESSA} and \texttt{FATODE} (discrete adjoints) \cite{FATODE2014} both in Fortram; 
\texttt{SciMLSensitivity.jl} in Julia \cite{rackauckas2020universal}; 
\texttt{Dolfin-adjoint} based on the \texttt{FEniCS} Project \cite{dolfin2013, dolfin2018};
\texttt{DENSERKS} in Fortram\cite{alexe2007denserks}; 
\texttt{DASPKADJOINT} \cite{Cao_Li_Petzold_2002};
and \texttt{Diffrax} in Python\cite{kidger2021on}. 

It is important to remark that the underlying machinery of all solvers relies on solvers for linear systems of equations, which can be solved in dense, band (sparse), and Krylov mode. 
% This implies that methods based on numerical solvers are, in principle, more difficult to implement but also more efficient in computing gradients for complex differential equations. 
Another important consideration is that all these methods have subroutines to compute the VJPs involved in the sensitivity and adjoint equations. 
This calculation is carried out by another sensitivity method, usually finite differences or AD, which plays a central role when analyzing the accuracy and stability of the adjoint method. 

\subsubsection{Forward sensitivity equation}
\label{section:computing-sensitivity-equations}

For systems of equations with few number of parameters, this method is useful since the system of $n(p+1)$ equations composed by Equations \eqref{eq:original_ODE} and \eqref{eq:sensitivity_equations} can be solved using the same precision for both solution and sensitivity numerical evaluation. 
Furthermore, it does not required saving the solution in memory. 
The following example illustrates how Equation \eqref{eq:example-ode-direct-methods} and the sensitivity equation can be solved simultaneously using the simple explicit Euler method $\clubsuit_\text{\code{code:sensitivity-equation}}$: 
\begin{jllisting}
ω = 0.2
p = [ω]
u0 = [0.0, 1.0]
tspan = [0.0, 10.0]

# Dynamics
function f(u, p, t)
    du₁ = u[2]
    du₂ = - p[1]^2 * u[1]
    return [du₁, du₂]
end

# Jacobian ∂f/∂p
function ∂f∂p(u, p, t)
    Jac = zeros(length(u), length(p))
    Jac[2,1] = -2*p[1]*u[1]
    return Jac
end

# Jacobian ∂f/∂u
function ∂f∂u(u, p, t)
    Jac = zeros(length(u), length(u))
    Jac[1,2] = 1
    Jac[2,1] = -p[1]^2
    return Jac
end

# Explicit Euler method
function sensitivityequation(u0, tspan, p, dt)
    u = u0
    sensitivity = zeros(length(u), length(p))
    for ti in tspan[1]:dt:tspan[2]
        sensitivity += dt * (∂f∂u(u, p, ti) * sensitivity + ∂f∂p(u, p, ti))
        u += dt * f(u, p, ti) 
    end
    return u, sensitivity   
end

u, s = sensitivityequation(u0, tspan , p, 0.001)
\end{jllisting}
The simplicity of the sensitivity method makes it available in most software for sensitivity analysis. 
In Julia, the \texttt{ODEForwardSensitivityProblem} method implements continuous sensitivity analysis, which generates the JVP operations required as part of the forward sensitivity equations via \texttt{ForwardDiff.jl} (see Section \ref{section:forwardAD-sensitivity}) or finite differences.
The same result can be achieved by:
\begin{jllisting}
using Zygote, SciMLSensitivity

function f!(du, u, p, t)
    du[1] = u[2]
    du[2] = - p[1]^2 * u[1]
end

prob = ODEForwardSensitivityProblem(f!, u0, tspan, p)
sol = solve(prob, Tsit5())
\end{jllisting}
Notice that in the last example we needed to re-defined the out-of-place function \texttt{f} to the in-place version \texttt{f!}.

For stiff systems of ODEs the use of the sensitivity equations can be computationally unfeasible \cite{kim_stiff_2021}.
This is because stiff ODEs require the use of stable solvers with cubic cost with respect to the number of ODEs\cite{hairer-solving-2}, making the total complexity of the sensitivity method $\mathcal{O}(n^3p^3)$. 
This complexity makes this method expensive for models with many ODEs and/or parameters unless the solver is able to further specialize on sparsity or properties of the linear solver (i.e. through Newton-Krylov methods). 

\paragraph{Computing VJPs inside the solver}
\label{section:computing-vjp-inside-solver}

All solver-based methods require the computations of VJPs. 
In the case of the sensitivity equation, this correspond to the row/column terms in $\frac{\partial f}{\partial u} s $ in Equation \eqref{eq:sensitivity_equations}.
For the adjoint equations, although an efficient trick has been used to remove the computationally expensive VJP, we still need to evaluate the term $\lambda^T \frac{\partial G}{\partial \theta}$ for the discrete adjoint method in Equation \eqref{eq:def_adjoint}, and $\lambda^T \frac{\partial f}{\partial \theta}$ for the continuous adjoint method in Equation \eqref{eq:casa-final-loss-gradient}. 
% Mention why these are different?
Therefore, the choice of the algorithm to compute VJPs can have a significant impact in the overall performance. 

In \texttt{SUNDIALS}, the VJPs involved in the sensitivity and adjoint method are handled using finite differences unless specified by the user \cite{SUNDIALS-hindmarsh2005sundials}.
In \texttt{FATODE}, these can be computed with finite differences, AD, or it can be provided by the user.
In the Julia ecosystem, different AD packages are available for this task (see Section \ref{sec:software-reverse-AD}), including \texttt{ForwardDiff.jl}, \texttt{ReverseDiff.jl}, \texttt{Zygote.jl} \cite{Innes_Zygote}, \texttt{Enzyme.jl} \cite{moses_Enzyme}, \texttt{Tracker.jl}.
These will compute the VJPs using some form of AD, which will result in correct calculations but potentially sub-optimal code. 
In Julia, the options \texttt{autodiff} and \texttt{autojacvec} allow to customized if VJPs are computed using AD or finite differences.  
% For these cases, customized VJPs function can be passed to the sensitivity methods using the \texttt{autojacvec}. 

\subsubsection{Adjoint methods}
\label{section:computing-adjoints}

% Distinctio between discrete and continuous
% Discrete are the exact gradient of the computer program, continuous are not. 
% There is no flexibilty in discrete adjoitns, but there is in continuous
% Human effort required to compute discrete adjoints is large \cite{FATODE2014}

For complex and large systems, direct methods for computing the gradient on top of the numerical solver can be memory expensive due to the large number of function evaluations required by the solver and the later store of the intermediate states. 
For these cases, adjoint-based methods allows us to compute the gradients of a loss function by instead computing the adjoint that serves as a bridge between the solution of the ODE and the final gradient. 
If done well the adjoint method offers considerate advantages when working with complex systems, since we are dealing with a new differential equation special care needs to be taken with respect to numerical efficiency and stability.

\paragraph{Discrete adjoint method}

In order to illustrate how the discrete adjoint method works, the following example shows how to manually solve for the gradient of the solution of \eqref{eq:example-ode-direct-methods} using an explicit Euler method. 
\begin{jllisting}
function discrete_adjoint_method(u0, tspan, p, dt)
    u = u0
    times = tspan[1]:dt:tspan[2]

    λ = [1.0, 0.0]
    ∂L∂θ = zeros(length(p))
    u_store = [u]

    # Forward pass to compute solution
    for t in times[1:end-1]
        u += dt * f(u, p, t)
        push!(u_store, u)
    end

    # Reverse pass to compute adjoint
    for (i, t) in enumerate(reverse(times)[2:end])
        u_memory = u_store[end-i+1]
        λ += dt * ∂f∂u(u_memory, p, t)' * λ
        ∂L∂θ += dt * λ' * ∂f∂p(u_memory, p, t)
    end

    return ∂L∂θ
end

∂L∂θ = discrete_adjoint_method(u0, tspan, p, 0.001) 
\end{jllisting}
In this case, the full solution in the forward pass is stored in memory and used to compute the adjoint and integrate the loss function during the reverse pass. 
As in the case of reverse AD, checkpointing can be used here. 

The previous example shows a manual implementation of the adjoint method. 
However, as we discuss in Section \ref{section:comparison-discrete-adjoint-AD}, the discrete adjoint method can be directly been implemented using reverse AD. 
In the Julia SciML ecosystem, \texttt{ReverseDiffAdjoint} performs reverse AD on the numerical solver via \texttt{ReverseDiff.jl}; \texttt{ZygoteAdjoint} via \texttt{Zygote.jl}; and \texttt{TrackerAdjoint} via \texttt{Tracker.jl}. 
% In all these cases, a custom pullback function needs to be specified that specifies how VJPs are computed thought the numerical solver \cite{rackauckas2021generalized}.

% Mention that in many cases this is implemented manually

\paragraph{Continuous adjoint method}

\begin{table}[bt]
\centering
\setlength{\tabcolsep}{10pt} % Default value: 6pt
\renewcommand{\arraystretch}{1.5} % Default value: 1
% \footnotesize
% \scriptsize
\begin{tabular}{ c |c c c c|} 
 \cline{2-5}
 &\textbf{Method} & \textbf{Stability} & \textbf{Stiff Performance} & \textbf{Memory} 
 \\ [0.5ex] 
 \cline{2-5}
 \hline
 \multirow{3}*{\rotatebox{90}{\textbf{Discrete}}}  
 &ReverseDiffAdjoint & Good & $\mathcal O (n^3 + p)$ & High \\
 &ZygoteAdjoint & Good & $\mathcal O (n^3 + p)$ & High \\
 &TrackerAdjoint & Good & $\mathcal O (n^3 + p)$ & High
 \\ [0.5ex] 
 \hline\hline
 \multirow{7}*{\rotatebox{90}{\textbf{Continuous}}} 
 & Sensitivity equation & Good & $\mathcal O (n^3p^3)$ & $\mathcal O(1)$ \\
 \cline{2-5}
 &Backsolve adjoint & Poor & $\mathcal O ((n+p)^3)$ & $\mathcal O(1)$ \\ 
 &Backsolve adjoint$^\blacktriangleleft$ & Medium & $\mathcal O ((n+p)^3)$ & $\mathcal O (K)$ \\
 &Interpolating adjoint & Good & $\mathcal O ((n+p)^3)$ & High \\ 
 &Interpolating adjoint$^\blacktriangleleft$ & Good & $\mathcal O ((n+p)^3)$ & $\mathcal O (K)$ \\
 &Quadrature adjoint & Good & $\mathcal O (n^3 + p)$ & High \\
 &Gauss adjoint$^\blacktriangleleft$ & Good & $\mathcal O (n^3 + p)$ & High \\
  &Gauss adjoint & Good & $\mathcal O (n^3 + p)$ & $O(K)$ \\
 % name & .. & .. & .. \\
 \hline
\end{tabular}
% \vspace
\caption{Comparison in performance and cost of solver-based methods. Methods that can be checkpointed are indicated with the symbol $\blacktriangleleft$, with $K$ the total number of checkpoints.}
\label{table:adjoint}
\end{table}

The continuous adjoint methods offers a series of advantages over the discrete method and the rest of the forward methods previously discussed. 
Just as the discrete adjoint methods and backpropagation, the bottleneck is how to solve for the adjoint $\lambda(t)$ due to its dependency with VJPs involving the state $u(t)$.
Effectively, notice that Equation \eqref{eq:casa-adjoint-equation} involves the terms $f(u, \theta, t)$ and $\frac{\partial h}{\partial u}$, which are both functions of $u(t)$. 
In opposition to the discrete adjoint methods, notice that here the full continuous trajectory $u(t)$ is needed, instead of its discrete pointwise evaluation. 
There are two solutions for addressing the evaluation of $u(t)$ during the backwards step.
\begin{enumerate}[label=(\roman*)]
    \item \textbf{Interpolation.} During the forward model, we can store in memory intermediate states of the numerical solution allowing the dense evaluation of the numerical solution at any given time. 
    % This leads to heavy-memory expensive algorithms. 
    % However, in opposition to discrete methods, solving for the adjoint in reverse mode requires to be able to evaluate the solution $u(t)$ at any given time $t$. 
    This can be done using dense output formulas, for example by adding extra stages to the Runge-Kutta scheme (Equation \eqref{eq:Runge-Kutta-scheme}) that allows to define a continuous interpolation, a method known as continuous Runge-Kutta \cite{hairer-solving-2, Alexe_Sandu_2009}. 
    % When using checkpointing, intermediate variables are saved and the interpolation between them is re-computed on demand. 
    \item \textbf{Backsolve.} Solve again the original system of ODEs together with the adjoint as the solution of the reversed augmented system \cite{chen_neural_2019}
    \begin{equation}
    \frac{d}{dt}
    \begin{bmatrix}
       u \\
       \lambda \\
       \frac{dL}{d\theta}
    \end{bmatrix}
    = 
    \begin{bmatrix}
       -f \\
       - \frac{\partial f}{\partial u}^T \lambda - \frac{\partial h}{\partial u}^T \\
       - \lambda^T \frac{\partial f}{\partial \theta} - \frac{\partial h}{\partial \theta}
    \end{bmatrix}
    % = 
    % - [ 1, \lambda^T, \lambda^T ]
    % \begin{bmatrix}
    %    f & \frac{\partial f}{\partial u} & \frac{\partial f}{\partial \theta} \\
    %    0 & 0 & 0 \\
    %    0 & 0 & 0
    % \end{bmatrix},
    \qquad 
    \begin{bmatrix}
       u \\
       \lambda \\
       \frac{dL}{d\theta}
    \end{bmatrix}(t_1)
    = 
    \begin{bmatrix}
       u(t_1) \\
       \frac{\partial L}{\partial u(t_1)} \\
       \lambda(t_0)^T s(t_0)
    \end{bmatrix}.
    \end{equation}
    An important problem with this approach is that computing the ODE backwards $\frac{du}{dt} = - f(u,\theta, t)$ can be unstable and lead to large numerical errors \cite{kim_stiff_2021, Zhuang_2020}. 
    One way of solving this system of equations that ensures stability is by using implicit methods. 
    However, this requires cubic time in the total number of ordinary differential equations, leading to a total complexity of $\mathcal O((n+p)^3)$ for the adjoint method.
\end{enumerate} 

When dealing with stiff differential equations, special considerations need to be taken into account.
Two alternatives are proposed in \cite{kim_stiff_2021}, the first referred to as \textit{Quadrature Adjoint} produces a high order interpolation of the solution $u(t)$ as we move forward, then solve for $\lambda$ backwards using an implicit solver and finally integrating $\frac{dL}{d\theta}$ in a forward step.
This reduces the complexity to $\mathcal O (n^3 + p)$, where the cubic cost in the number of ODEs comes from the fact that we still need to solve the original stiff differential equation in the forward step. 
A second similar approach is to use an implicit-explicit (IMEX) solver, where we use the implicit part for the original equation and the explicit for the adjoint. 
This method also has a complexity of $\mathcal O (n^3 + p)$.

\paragraph{Continuous checkpointing}

Both interpolating and backsolve adjoint methods can be implemented along with a checkpointing scheme. 
This can be done by choosing save points in the forward pass. 
For the interpolating methods, the interpolation is reconstructed in the backwards pass between two save points. 
This reduces the total memory requirement of the interpolating method to simply the maximum cost of holding an interpolation between two save points, but requires a total additional computational effort equal to one additional forward pass. 
In the backsolve variation, the value $u$ in the reverse pass can be corrected to be the saved point, thus resetting the numerical error introduced during the backwards evaluation and thus improving the accuracy.

\paragraph{Solving the quadrature}

Another computational consideration is how the integral in Equation \eqref{eq:casa-final-loss-gradient} is numerically evaluated. While one can solve the integral simultaneously to the other equations using an ODE solver, this is only recommended with explicit methods as with implicit methods these additional ODEs are of size $p$ and thus can increase the complexity of an implicit solve by $O(p^3)$. The InterpolationgAdjoint and BacksolveAdjoint method use this ODE solver approach for computing the integrand. The QuadratureAdjoint approach avoids this computational cost by computing the dense solution $\lambda(t)$ and performs the quadrature after the solution using an adaptive Gauss-Kronrod method (QuadGK.jl). This method results in global error control on the integration and removes the cubic scaling within implicit solvers but requires a larger memory cost by storing the adjoint pass continuous solution.

Solvers designed for large implicit systems allow for solving explicit integrals based on the ODE solution simultaneously without including the equations in the ODE evaluation in order to avoid this expense. The Sundials COVDE solver introduced this technique specifically for BDF methods \cite{SUNDIALS-hindmarsh2005sundials}. In the Julia DifferentialEquations.jl solvers, this can be done using a callback (specifically the numerical integration callbacks form the DiffEqCallbacks.jl library). The GaussAdjoint method uses the callback approach to allow for a simultanious explicit evaluation the integral using Gaussian quadrature (related to \cite{Norcliffe_gaussquadrature_2023} but using a different approximation to improve convergence).

These differences in the schemes for computing $u(t)$ and the final quadrature give rise to the set of methods in Table \ref{table:adjoint}. All of these are ''the adjoint method`` with differences being in the way steps of the adjoint method are approximated, and notably there is a general trade-off in stability, performance, and memory. One final piece to note is that while the GaussAdjoint achieves good properties according to this chart, the QuadratureAdjoint notably uses a global error control of the quadrature as opposed to the local error control of the GaussAdjoint, and thus can achieve more robust bounding of the error with respect to user chosen tolerances.

