
% Sensitivity methods based on numerical solvers tend to be better adapted to the structure and properties of the underlying ODE (stiffness, stability, accuracy) but are also more difficult to implement.  
% This difficulty arises from the fact that the sensitivity methods need to deal with some numerical and computational considerations, including
Sensitivity methods based on numerical solvers need to deal with some numerical and computational considerations, including
\begin{enumerate}[label=(\roman*)]
    \item How to handle matrix/Jacobian-vector products
    \item Numerical stability of the forward/reverse solver
    \item Memory-time tradeoff
\end{enumerate}
These factors are further exacerbated by the number of ODEs and parameters in the model. 
% While explicit methods can be preferable for non-stiff problems, Rosenblock methods can be 
Just a few modern scientific software have the capabilities of solving forward ODE and computing their sensitivities at the same time. 
These include 
\texttt{CVODES} within \texttt{SUNDIALS} in C \cite{serban2005cvodes, SUNDIALS-hindmarsh2005sundials}; 
\texttt{ODESSA} \cite{ODESSA} and \texttt{FATODE} (discrete adjoints) \cite{FATODE2014} both in Fortram; 
\texttt{SciMLSensitivity.jl} in Julia \cite{rackauckas2020universal}; 
\texttt{Dolfin-adjoint} based on the \texttt{FEniCS} Project \cite{dolfin2013, dolfin2018};
\texttt{DENSERKS} in Fortran \cite{alexe2007denserks}; 
\texttt{DASPKADJOINT} \cite{Cao_Li_Petzold_2002};
and \texttt{Diffrax} \cite{kidger2021on} and \texttt{torchdiffeq} \cite{torchdiffeq} in Python. 

It is important to remark that the underlying machinery of all solvers relies on solvers for linear systems of equations, which can be solved in dense, band (sparse), and Krylov mode. 
% This implies that methods based on numerical solvers are, in principle, more difficult to implement but also more efficient in computing gradients for complex differential equations. 
Another important consideration is that all these methods have subroutines to compute the JVPs and VJPs involved in the sensitivity and adjoint equations, respectively. 
This calculation is carried out by another sensitivity method, usually finite differences or AD, which plays a central role when analyzing the accuracy and stability of the adjoint method. 

\subsubsection{Forward sensitivity equation}
\label{section:computing-sensitivity-equations}

For systems of equations with few number of parameters, this method is useful since the system of $n(p+1)$ equations composed by Equations \eqref{eq:original_ODE} and \eqref{eq:sensitivity_equations} can be solved using the same precision for both solution and sensitivity numerical evaluation. 
Furthermore, it does not required saving the solution in memory. 
The following example illustrates how Equation \eqref{eq:example-ode-direct-methods} and the sensitivity equation can be solved simultaneously using the simple explicit Euler method $\clubsuit_\text{\code{code:sensitivity-equation}}$: 
\begin{jllisting}
ω = 0.2
p = [ω]
u0 = [0.0, 1.0]
tspan = [0.0, 10.0]

# Dynamics
function f(u, p, t)
    du₁ = u[2]
    du₂ = - p[1]^2 * u[1]
    return [du₁, du₂]
end

# Jacobian ∂f/∂p
function ∂f∂p(u, p, t)
    Jac = zeros(length(u), length(p))
    Jac[2,1] = -2*p[1]*u[1]
    return Jac
end

# Jacobian ∂f/∂u
function ∂f∂u(u, p, t)
    Jac = zeros(length(u), length(u))
    Jac[1,2] = 1
    Jac[2,1] = -p[1]^2
    return Jac
end

# Explicit Euler method
function sensitivityequation(u0, tspan, p, dt)
    u = u0
    sensitivity = zeros(length(u), length(p))
    for ti in tspan[1]:dt:tspan[2]
        sensitivity += dt * (∂f∂u(u, p, ti) * sensitivity + ∂f∂p(u, p, ti))
        u += dt * f(u, p, ti) 
    end
    return u, sensitivity   
end

u, s = sensitivityequation(u0, tspan , p, 0.001)
\end{jllisting}
The simplicity of the sensitivity method makes it available in most software for sensitivity analysis. 
In \texttt{SciMLSensitivity} in the Julia SciML ecosystem, the \texttt{ODEForwardSensitivityProblem} method implements continuous sensitivity analysis, which generates the JVPs required as part of the forward sensitivity equations via \texttt{ForwardDiff.jl} (see Section \ref{section:forwardAD-sensitivity}) or finite differences.
Using \texttt{SciMLSensitivity} reduces the code above to
\begin{jllisting}
using SciMLSensitivity

function f!(du, u, p, t)
    du[1] = u[2]
    du[2] = - p[1]^2 * u[1]
end

prob = ODEForwardSensitivityProblem(f!, u0, tspan, p)
sol = solve(prob, Tsit5())
\end{jllisting}
Notice that in the last example we needed to re-define the out-of-place function \texttt{f} to the in-place version \texttt{f!}.

For stiff systems of ODEs the use of the forward sensitivity equations can be computationally unfeasible \cite{kim_stiff_2021}.
This is because stiff ODEs require the use of stable solvers with cubic cost with respect to the number of ODEs \cite{hairer-solving-2}, making the total complexity of the sensitivity method $\mathcal{O}(n^3p^3)$. 
This complexity makes this method expensive for models with many ODEs and/or parameters unless the solver is able to further specialize on sparsity or properties of the linear solver (i.e. through Newton-Krylov methods). 

\paragraph{Computing JVPs and VJPs inside the solver}
\label{section:computing-vjp-inside-solver}

All solver-based methods require the computation of JVPs or VJPs. 
In the case of the sensitivity equation, this correspond to the JVPs resulting form the product $\frac{\partial f}{\partial u} s $ in Equation \eqref{eq:sensitivity_equations}.
For the adjoint equations, although an efficient trick has been used to remove the computationally expensive VJP, we still need to evaluate the term $\lambda^T \frac{\partial G}{\partial \theta}$ for the discrete adjoint method in Equation \eqref{eq:def_adjoint}, and $\lambda^T \frac{\partial f}{\partial \theta}$ for the continuous adjoint method in Equation \eqref{eq:casa-final-loss-gradient}. 
% Mention why these are different?
Therefore, the choice of the algorithm to compute JVPS/VJPs can have a significant impact in the overall performance \cite{Schäfer_Tarek_White_Rackauckas_2021}. 

In \texttt{SUNDIALS}, the JVPs/VJPs involved in the sensitivity and adjoint method are handled using finite differences unless specified by the user \cite{SUNDIALS-hindmarsh2005sundials}.
In \texttt{FATODE}, they can be computed with finite differences, AD, or it can be provided by the user.
In the the Julia SciML ecosystem, different AD packages are available for this task (see Section \ref{sec:software-reverse-AD}), including \texttt{ForwardDiff.jl}, \texttt{ReverseDiff.jl}, \texttt{Zygote.jl} \cite{Innes_Zygote}, \texttt{Enzyme.jl} \cite{moses_Enzyme}, \texttt{Tracker.jl}.
These will compute the JVPs/VJPs using some form of AD, which will result in correct calculations but potentially sub-optimal code. 
In Julia, the options \texttt{autodiff} and \texttt{autojacvec} allow one to customize if JVPs/VJPs are computed using AD or finite differences.  
% For these cases, customized VJPs function can be passed to the sensitivity methods using the \texttt{autojacvec}. 

\subsubsection{Adjoint methods}
\label{section:computing-adjoints}

For complex and large systems (e.g. of more than 50 parameters and ODEs), direct methods for computing the gradient on top of the numerical solver can be memory expensive due to the large number of function evaluations required by the solver and the later store of the intermediate states. 
For these cases, adjoint-based methods allows us to compute the gradients of a loss function by instead computing the adjoint that serves as a bridge between the solution of the ODE and the final gradient. 
If done well the adjoint method offers considerate advantages when working with complex systems.
Since adjoint methods rely on an additional set of differential equations which are solved, numerical efficiency and stability must be investigated.

\paragraph{Discrete adjoint method}

In order to illustrate how the discrete adjoint method may be implemented, the following example shows how to manually solve for the gradient of the solution of \eqref{eq:example-ode-direct-methods} using an explicit Euler method. 
\begin{jllisting}
function discrete_adjoint_method(u0, tspan, p, dt)
    u = u0
    times = tspan[1]:dt:tspan[2]

    λ = [1.0, 0.0]
    ∂L∂θ = zeros(length(p))
    u_store = [u]

    # Forward pass to compute solution
    for t in times[1:end-1]
        u += dt * f(u, p, t)
        push!(u_store, u)
    end

    # Reverse pass to compute adjoint
    for (i, t) in enumerate(reverse(times)[2:end])
        u_memory = u_store[end-i+1]
        λ += dt * ∂f∂u(u_memory, p, t)' * λ
        ∂L∂θ += dt * λ' * ∂f∂p(u_memory, p, t)
    end

    return ∂L∂θ
end

∂L∂θ = discrete_adjoint_method(u0, tspan, p, 0.001) 
\end{jllisting}
In this case, the full solution in the forward pass is stored in memory and used to compute the adjoint and integrate the loss function during the reverse pass. 
As in the case of reverse AD, checkpointing can be used here. 

While the previous example illustrates a manual implementation of the adjoint method, the discrete adjoint method can be directly implemented using reverse AD (Section \ref{section:comparison-discrete-adjoint-AD}).
In the Julia SciML ecosystem, \texttt{ReverseDiffAdjoint} performs reverse AD on the numerical solver via \texttt{ReverseDiff.jl}; \texttt{ZygoteAdjoint} via \texttt{Zygote.jl}; and \texttt{TrackerAdjoint} via \texttt{Tracker.jl}. 

% Mention that in many cases this is implemented manually

\paragraph{Continuous adjoint method}

\begin{table}[bt]
\centering
\setlength{\tabcolsep}{6pt} % Default value: 6pt
\renewcommand{\arraystretch}{1.5} % Default value: 1
\small
\begin{tabular}{ c |c c c c c|} 
 \cline{2-6}
 &\textbf{Method} & \textbf{Stability} & \textbf{Non-Stiff Performance} & \textbf{Stiff Performance} & \textbf{Memory} 
 \\ [0.5ex] 
 \cline{2-6}
 \hline
 \multirow{3}*{\rotatebox{90}{\textbf{Discrete}}}  
 &ReverseDiffAdjoint & Good & & $\mathcal O (n^3 + p)$ & High \\
 &ZygoteAdjoint & Good & & $\mathcal O (n^3 + p)$ & High \\
 &TrackerAdjoint & Good & & $\mathcal O (n^3 + p)$ & High
 \\ [0.5ex] 
 \hline\hline
 \multirow{7}*{\rotatebox{90}{\textbf{Continuous}}} 
 & Sensitivity equation & Good & & $\mathcal O (n^3p^3)$ & $\mathcal O(1)$ \\
 \cline{2-6}
 &Backsolve adjoint & Poor & & $\mathcal O ((n+p)^3)$ & $\mathcal O(1)$ \\ 
 &Backsolve adjoint$^\blacktriangleleft$ & Medium & & $\mathcal O ((n+p)^3)$ & $\mathcal O (K)$ \\
 &Interpolating adjoint & Good & & $\mathcal O ((n+p)^3)$ & High \\ 
 &Interpolating adjoint$^\blacktriangleleft$ & Good & & $\mathcal O ((n+p)^3)$ & $\mathcal O (K)$ \\
 &Quadrature adjoint & Good & & $\mathcal O (n^3 + p)$ & High \\
 &Gauss adjoint & Good & & $\mathcal O (n^3 + p)$ & High \\
 &Gauss adjoint$^\blacktriangleleft$ & Good & & $\mathcal O (n^3 + p)$ & $\mathcal O(K)$ \\
 % name & .. & .. & .. \\
 \hline
\end{tabular}
% \vspace
\caption{Comparison in performance and cost of solver-based methods. Methods that can be checkpointed are indicated with the symbol $\blacktriangleleft$, with $K$ the total number of checkpoints. The nomenclature of the different adjoint methods here follows the naming in the documentation of \texttt{SciMLSensitivity.jl} \cite{rackauckas2020universal}.}
\label{table:adjoint}
\end{table}

The continuous adjoint methods offers a series of advantages over the discrete method and the rest of the forward methods previously discussed. 
Just as the discrete adjoint methods and backpropagation, the bottleneck is how to solve for the adjoint $\lambda(t)$ due to its dependency with VJPs involving the state $u(t)$.
Effectively, notice that Equation \eqref{eq:casa-adjoint-equation} involves the terms $f(u, \theta, t)$ and $\frac{\partial h}{\partial u}$, which are both functions of $u(t)$. 
However, in contrast to the discrete adjoint methods, here the full continuous trajectory $u(t)$ is needed, instead of its discrete pointwise evaluation. 
There are two solutions for addressing the evaluation of $u(t)$ during the backwards step.
\begin{enumerate}[label=(\roman*)]
    \item \textbf{Interpolation.} During the forward model, we can store in memory intermediate states of the numerical solution allowing the dense evaluation of the numerical solution at any given time. 
    % This leads to heavy-memory expensive algorithms. 
    % However, in opposition to discrete methods, solving for the adjoint in reverse mode requires to be able to evaluate the solution $u(t)$ at any given time $t$. 
    This can be done using dense output formulas, for example by adding extra stages to the Runge-Kutta scheme (Equation \eqref{eq:Runge-Kutta-scheme}) that allows to define a continuous interpolation, a method known as continuous Runge-Kutta \cite{hairer-solving-2, Alexe_Sandu_2009}. 
    % When using checkpointing, intermediate variables are saved and the interpolation between them is re-computed on demand. 
    \item \textbf{Backsolve.} Solve again the original system of ODEs together with the adjoint as the solution of the reversed augmented system \cite{chen_neural_2019}
    \begin{equation}
    \frac{d}{dt}
    \begin{bmatrix}
       u \\
       \lambda \\
       \frac{dL}{d\theta}
    \end{bmatrix}
    = 
    \begin{bmatrix}
       -f \\
       - \frac{\partial f}{\partial u}^T \lambda - \frac{\partial h}{\partial u}^T \\
       - \lambda^T \frac{\partial f}{\partial \theta} - \frac{\partial h}{\partial \theta}
    \end{bmatrix}
    % = 
    % - [ 1, \lambda^T, \lambda^T ]
    % \begin{bmatrix}
    %    f & \frac{\partial f}{\partial u} & \frac{\partial f}{\partial \theta} \\
    %    0 & 0 & 0 \\
    %    0 & 0 & 0
    % \end{bmatrix},
    \qquad 
    \begin{bmatrix}
       u \\
       \lambda \\
       \frac{dL}{d\theta}
    \end{bmatrix}(t_1)
    = 
    \begin{bmatrix}
       u(t_1) \\
       \frac{\partial L}{\partial u(t_1)} \\
       \lambda(t_0)^T s(t_0)
    \end{bmatrix}.
    \end{equation}
    An important problem with this approach is that computing the ODE backwards (the differential equation $\frac{du}{dt} = - f(u,\theta, t)$) can be unstable and lead to large numerical errors \cite{kim_stiff_2021, Zhuang_2020}. 
    Implicit methods may be used to ensure stability when solving this system of equations. 
    However, this requires cubic time in the total number of ordinary differential equations, leading to a total complexity of $\mathcal O((n+p)^3)$ for the adjoint method. In practice, this method is hardly stable for most complex (even non-stiff) differential equations. 
\end{enumerate} 

When dealing with stiff differential equations, special considerations need to be taken into account.
Two alternatives are proposed in \cite{kim_stiff_2021}, the first referred to as \textit{Quadrature Adjoint} produces a high order interpolation of the solution $u(t)$, then solve for $\lambda$ in reverse using an implicit solver and finally integrating $\frac{dL}{d\theta}$ in a forward step.
This reduces the complexity to $\mathcal O (n^3 + p)$, where the cubic cost in the number of ODEs comes from the fact that we still need to solve the original stiff differential equation in the forward step. 
A second similar approach is to use an implicit-explicit (IMEX) solver, where we use the implicit part for the original equation and the explicit for the adjoint. 
This method also has a complexity of $\mathcal O (n^3 + p)$. 

\paragraph{Continuous checkpointing}

Both interpolating and backsolve adjoint methods can be implemented along with a checkpointing scheme. 
This can be done by choosing save points in the forward pass. 
For the interpolating methods, the interpolation is reconstructed in the backwards pass between two save points. 
This reduces the total memory requirement of the interpolating method to simply the maximum cost of holding an interpolation between two save points, but requires a total additional computational effort equal to one additional forward pass. 
In the backsolve variation, the value $u$ in the reverse pass can be corrected to be the saved point, thus resetting the numerical error introduced during the backwards evaluation and thus improving the accuracy.

\paragraph{Solving the quadrature}

Another computational consideration is how the integral in Equation \eqref{eq:casa-final-loss-gradient} is numerically evaluated. 
While one can solve the integral simultaneously to the other equations using an ODE solver, this is only recommended with explicit methods as with implicit methods these additional ODEs are of size $p$ and thus can increase the complexity of an implicit solve by $O(p^3)$. 
The interpolating adjoint and backsolve adjoint method use this ODE solver approach for computing the integrand. 
On the other side, the quadrature adjoint approach avoids this computational cost by computing the dense solution $\lambda(t)$ and then computing the quadrature  
\begin{align}
    \frac{dL}{d\theta}
    &= 
    \lambda(t_0)^T s(t_0)
    + 
    \inttime
    \left( \frac{\partial h}{\partial \theta} + \lambda^T \frac{\partial f}{\partial \theta} \right) dt \nonumber \\
    &\approx
    \lambda(t_0)^T s(t_0)
    + 
    \sum_{j=1}^N \omega_j \left( \frac{\partial h}{\partial \theta} + \lambda^T \frac{\partial f}{\partial \theta} \right) (\tau_i)
\end{align}
where $\omega_i$, $\tau_i$ are the weights and knots of a Gauss-Kronrod quadrature method for numerical integration from \texttt{QuadGK.jl} \cite{laurie1997calculation, gonnet2012review}. 
This method results in global error control on the integration and removes the cubic scaling within implicit solvers. Nonetheless, it requires a larger memory cost by storing the adjoint pass continuous solution.

Solvers designed for large implicit systems allow for solving explicit integrals based on the ODE solution simultaneously without including the equations in the ODE evaluation in order to avoid this expense. 
The Sundials COVDE solver introduced this technique specifically for BDF methods \cite{SUNDIALS-hindmarsh2005sundials}. 
In the Julia \texttt{DifferentialEquations.jl} solvers, this can be done using a callback (specifically the numerical integration callbacks form the \texttt{DiffEqCallbacks.jl} library). 
The Gauss adjoint method uses the callback approach to allow for a simultaneous explicit evaluation the integral using Gaussian quadrature, similar to \cite{Norcliffe_gaussquadrature_2023} but using a different approximation to improve convergence.

These differences in the schemes for computing $u(t)$ and the final quadrature give rise to the set of methods in Table \ref{table:adjoint}. 
Excluding the sensitivity equation which was added for completeness, all of these are \textit{the adjoint method} with differences being in the way steps of the adjoint method are approximated, and notably Table \ref{table:adjoint} shows a general trade-off in stability, performance, and memory across the methods. 
While the Gauss adjoint achieves good properties according to this chart, the quadrature adjoint notably uses a global error control of the quadrature as opposed to the local error control of the Gauss adjoint, and thus can achieve more robust bounding of the error with respect to user chosen tolerances.

