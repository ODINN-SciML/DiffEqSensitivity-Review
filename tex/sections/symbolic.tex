In symbolic differentiation, functions are represented algebraically instead of algorithmically, which is why many symbolic differentiation tools are included inside computer algebra systems (CAS) \cite{Symbolics_jl_2022}. 
Instead of numerically evaluating the final value of a derivative, symbolic systems assign variable names, expressions, operations, and literals to \textit{algebraic} objects. 
For example, the relation $y = x^2$ is interpreted as expression with two variables, $x$ and $y$, and the symbolic system generates the derivative $y' = 2 \times x$ with $2$ a numeric literal, $\times$ a binary operation, and $x$ the same variable assignment as in the original expression.

The general issue with symbolic differentiation is \textit{expression swell}, i.e. the fact that the size of a derivative expression can be much larger than the original expression \cite{Baydin_Pearlmutter_Radul_Siskind_2015}. 
One way to visualize this swell is to note that the product rule grows and expression of $f(x)g(x)$ into two expressions, namely $\frac{d}{dx}(f(x)g(x)) = \frac{df}{dx}g(x) + f(x)\frac{dg}{dx}$, and thus the composition of many functions leads to a large derivative expression. 
AD avoids expression swell by instead numerically calculating the derivative of a given expression at some fixed value, never representing the general derivative but only at the values obtained by the forward pass. 
This eager evaluation of the derivative around a given value forces the intermediate computation into the JVPs or VJPs form as a way to continually pass forward/reverse the current state. 
Meanwhile, symbolic differentiation can represent the complete derivative expression and thus avoid being forced into a given computation order, but at the memory cost of having to represent larger expressions.
% Simplification routines implemented in CAS may however reduce the size and complexity of algebraic expressions by finding common sub-expressions, making symbolic differentiation very efficient when computing derivatives multiple times and for different input values \cite{Dürrbaum_Klier_Hahn_2002}. 

However with this in mind it is important to acknowledge the close relationship between AD and symbolic differentiation.
AD uses symbolic differentiation in its definition of primitives which are then chained together in a specific way to form VJPs and vector products. 
Forward AD can be expressed as a form of symbolic differentiation with a specific choice of common subexpression elimination, i.e. forward mode can be expressed as a symbolic differentiation with a specific choice of how to accumulate the intermediate calculations so that expression growth can be avoided \cite{juedes1991taxonomy, Elliott_2018, Laue2020, Dürrbaum_Klier_Hahn_2002}.
However, general symbolic differentiation can have many other choices for the differentiation order, and does not in general require computation using the VJPs or JVPs \cite{Baydin_Pearlmutter_Radul_Siskind_2015}. 
This is apparent for example when computing sparse Jacobians, where generally symbolic differentiation computes entries element-by-element while forward mode automatic differentiation computes the matrix column-by-column and reverse mode computes row-by-row. 
Generally AD is recommended due to its generality on programs and the lack of potential memory issues by avoiding expression swell, though there are some situations where symbolic differentiation may be appropriate. 
For example, there are certain sparsity patterns by which even sparse AD requires computing the full Jacobian via all columns/rows, while computing the sparse Jacobian element-by-element only requires computing the non-zero elements.
